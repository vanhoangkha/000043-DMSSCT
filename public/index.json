[
{
	"uri": "/5-monitoring/5.1-cloudwatchmetrics/",
	"title": "CloudWatch Metrics",
	"tags": [],
	"description": "",
	"content": "AWS DMS provides you ability to monitor performance of your DMS migration via CloudWatch metrics. There are different metrics available at replication instance \u0026amp; task level.\nCloudWatch metrics for DMS Replication instance can be helpful in identifying resource consumption on DMS replication instance. CPU utilization, Freeable memory, Free Storage Space, etc. are few key metrics you may want to put monitoring on. CloudWatch metrics for DMS tasks can be helpful in identifying performance. DMS provides different metrics for Full Load, CDC, Validation \u0026amp; Impact on replication instance. Please refer AWS DMS Monitoring documentation for details on each metric.\nTo view CloudWatch metrics for DMS replication instance, Go to the AWS DMS console and click on Replication instances in the left side column. Select the replication instance and then navigate to CloudWatch metrics tab.\nTo view CloudWatch metrics for DMS tasks, Go to the AWS DMS console and click on Database Migration Task in the left side column. Click on your DMS task from the list of tasks.\nOnce you are on DMS task page, navigate to Monitoring tab.\nNext, we will learn about AWS DMS events and how to setup notifications for those events.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.1-configtarget/",
	"title": "Configure the Target Database",
	"tags": [],
	"description": "",
	"content": "Depending on your target schema, you will have to see one of the respective sub-sections below to follow the guide:\nConfigure the Target Database (Local Microsoft SQL Server to RDS Microsoft SQL Server)\rAurora (MySQL compatible) target\rAurora (PostgresSQL compatible) target\r"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.1-configtarget/a-rds-mssql/",
	"title": "Configure the Target Database (Local Microsoft SQL Server to RDS Microsoft SQL Server)",
	"tags": [],
	"description": "",
	"content": "\rThis guide is only for migration to RDS Microsoft SQL Server target. If your target database uses another schema, you can skip this page.\nNow on our EC2 instance we\u0026rsquo;ve been connecting to via RDP, we are going to connect to our SQL Server RDS target database, from within SQL Server Management Studio, select SQL Server Authentication and connect to the SQL Server RDS instance using the following parameters Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the SQL Server password. It is also on first Cloudformation Stack\u0026rsquo;s Output tab under SQL Server password :\nthen use information below to populate the login window\nParameter Value Server Type Database Engine Server Name \u0026lt; TargetSQLServerEndpoint (you can find this in Cloudformation Output tab or goto RDS service in console and pick your RDS SQL Server instance\u0026gt; Login dbadmin Password See Cloudformation Output tab \u0026amp; see value in DMSDBSecretP or look up password in AWS Secrets Manager for secret ending in RDSDBSecret Remeber Password check box checked it should look similar to\nOpen a New Query window if one doesn\u0026rsquo;t open.\nRun the following script to create a target database dms_sample_target on RDS SQL Server.\nuse master GO CREATE DATABASE dms_sample_target GO The target database (dms_sample_target) has now been created. You can return to the AWS Management Console.\n"
},
{
	"uri": "/2-selectsource/2.1-oracle/2.1.1-connectsrcdb/",
	"title": "Connect To The Source Oracle Database",
	"tags": [],
	"description": "",
	"content": " Once connected, open Oracle SQL Developer from the Taskbar. Note your icons maybe in a different order than the image below.\nClick on the plus sign from the left-hand menu to create a New Database Connection using the following values, Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the password. It is also on first Cloudformation Stack\u0026rsquo;s output tab (note it is the same if you\u0026rsquo;ve already gotten it from there) then click Connect.\nParameter Value Connection Name Source Oracle Username dbadmin Password See Cloudformation Output tab for the \u0026ldquo;cfn%\u0026rdquo; stack and obtain the value found for \u0026ldquo;DMSDBSecretP\u0026rdquo; or look it up in AWS Secrets Manager in the secret for Oracle Save Password Check Hostname \u0026lt; SourceOracleEndpoint (you can find this in Cloudformation Output tab or goto RDS service in console and pick your RDS Oracle Source instance)\u0026gt; Port 1521 SID ORACLEDB After the you see the test status as Successful, click Connect. Your window should look like below.\n"
},
{
	"uri": "/4-serverless/4.1-migrate/4.1.1-createmigration/",
	"title": "Creating a Serverless Replication",
	"tags": [],
	"description": "",
	"content": "In this step-by-step guide you will create Serverless Replication to migrate data from Oracle or SQL Server database to Amazon Aurora (MySQL or PostgreSQL compatible).\nThe environment for this lab consists of:\nA source database (SQL Server or Oracle).\nAn Amazon Aurora instance used as the target database.\nGo to AWS DMS Console. On the AWS DMS Console page, click Serverless replications. AWS DMS Serverless Replication is a new concept that abstracts the instances and tasks typically found in standard AWS DMS.\nOn the Serverless replications page, click the Create Replication button.\nConfigure Your Replication Provide a name to your Serverless replication and Configure replication to connect to your source and target databases. During this step, you\u0026rsquo;ll need to specify Source and Target endpoint. You\u0026rsquo;ll also configure the replication to perform a full load (migrating existing data) and enable Change Data Capture (CDC) to replicate ongoing changes from the source to the target database.\nParameter Value Task identifier **dms-serverless-lab** Source database endpoint Your source database endpoint (SQL Server or Oracle) , Example: **sql-server-source** Target database endpoint Your target database endpoint(MySQL or PostgreSQL) , Example: **mysql-target** Migration type Full Load and CDC ( Migrate existing data and replicate ongoing changes) Configure Additional Settings\nParameter Value Explanation Target table preparation mode Do Nothing\tThis ensures that any existing tables in the target database are not dropped and tables that are not present on target database are created Stop task after full load completes Don\u0026rsquo;t stop This option allows the replication to continue running even after the initial full load is completed, so it can handle ongoing changes. Include LOB columns in replication Limited LOB mode This setting ensures that large object (LOB) columns are included in the replication with certain limitations to optimize performance. Max LOB size 32 KB This specifies the maximum size for LOB columns that will be replicated. Turn on validation Keep it unchecked i.e. OFF Validation can be resource-intensive and is not always necessary for all scenarios. Turn on cloudwatch logs turn it on and keep all the \u0026ldquo;default\u0026rdquo;. This will allow you to monitor the replication process and troubleshoot any issues that arise. Set Up Table Selection Rules\nNote: Expand only for the source in your scenario. You can skip the other sources.\nFor Oracle source, use: Parameter Value Schema DMS_SAMPLE% Table name % Action Include For SQL Server source, use: Parameter Value Schema dbo% Table name % Action Include If the Create Task screen does not recognize any schemas, make sure to go back to endpoints screen and click on your endpoint. Scroll to the bottom of the page and click on Refresh Button (⟳) in the Schemas section. If your schemas still do not show up on the Create Task screen, click on the Guided tab and manually select ‘dbo’ schema and all tables.\nSet Up Transformation rules Transformation rules allow you to modify the structure or content of the data as it is migrated. Expand the Transformation rules section, and click on Add new transformation rule using the following values:\nFor Oracle source, use:\nRule 1: Parameter Value Target Schema Schema Name DMS_SAMPLE Action Make lowercase Rule 2: Parameter Value Target Table Schema Name DMS_SAMPLE Table Name % Action Make lowercase Rule 3: Parameter Value Target Column Schema Name DMS_SAMPLE Table Name % Column Name % Action Make lowercase For SQL Server Source, use:\nParameter Value Target Schema Schema Name dbo Action Rename to: dms_sample_target Specify Network related Settings\nUnder the Compute settings, choose the VPC and subnet where the replication will be created. Additionally, select the appropriate security group.\nParameter Value VPC Select VPC ID with DMSVpc in the name from drop down Subnet Group Choose the default one available in your VPC Security Groups Select the security group that starts with cfn-InstanceSecurityGroup-% AWS KMS Key **aws/dms** Select an Availability Zone\nUnder the Availability section, select the availability option suited to your workload and choose the specific AZ where your primary instance will operate. If you select \u0026ldquo;no preference\u0026rdquo;, AWS DMS will automatically choose the AZ for you.\nParameter Value Availability Single-AZ Availability Zone No Preference Select Minimum and Maximum DMS Capacity Units (DCU)\nUnder Capacity Settings, Select Minimum and Maximum DMS Capacity units (DCU). Replication compute capacity in AWS DMS is measured in DMS Capacity Units (DCU). Each DCU provides 2 GiB of memory along with corresponding compute and networking resources\nParameter Value Minimum DMS Capacity units (DCU) 1 Maximum DMS Capacity units (DCU) 4 Review and Create Replication\nReview all your configurations, and then initiate the Serverless replication by clicking Create Replication.\nIf all parameters were correct and validation was successful then the newly created replication will be visible under Serverless Replication.\nStart Replication\nTo begin migrating data from the source to the target database, start the serverless replication. Select the replication and, under Actions, choose Start.\nOn the Start replication page, Select Don\u0026rsquo;t stop CDC and then Click on Start Replication.\nThe replication process would immidiately start.\nThis concludes the replication creation process, and your replication is now starting. The Serverless replication process consists of 8 stages, which you can monitor under the Status column. Please proceed to the next page for a detailed overview of each stage.\n"
},
{
	"uri": "/",
	"title": "Database Schema Conversion &amp; Migration",
	"tags": [],
	"description": "",
	"content": "Database Schema Conversion \u0026amp; Migration Schema Conversion Converting the database schema and code objects is usually the most time-consuming operation in a heterogeneous database migration. By converting the schema properly, you can achieve a major milestone of the migration. The AWS Schema Conversion Tool (SCT) is an easy to use application that you can install on a local computer or an Amazon EC2 instance. At ReInvent 2023, AWS also added an in console experience in the Convert and Migrate section of the DMS service. Overtime we\u0026rsquo;ll be switching to the in console experience.\nSCT/Convert \u0026amp; Migrate in console helps simplify heterogeneous database migrations by examining your source database schema and automatically converting the majority of the database code objects, including views, stored procedures, and functions, to a format compatible with your new target database. Any objects that SCT cannot converted automatically is marked with detailed information that you can use to convert it manually.\nData Migration After you have completed the schema conversion, you\u0026rsquo;ll need to move the data itself. In case of production databases, you may not be able to afford very little downtime during the migration. Moreover, you may want to keep the transactions from source and target database in sync until you switch your application to the new target.\nThe AWS Database Migration Service helps you migrate the data from the source database to the target database easily and securely. AWS DMS supports data migration to and from most widely used commercial and open-source databases. The source database can be located in your on-premises environment, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database on Amazon EC2 or Amazon RDS. The target can also be non-relational like S3, Stream like Kafka/Kinesis, DocumentDB or DynamoDB etc. Additionally, the source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.\n"
},
{
	"uri": "/7-cleanup/7.1-deleteserverlesstask/",
	"title": "Delete the Serverless Migration Task",
	"tags": [],
	"description": "",
	"content": " Click here to go to the DMS console.\nOn the left-hand menu click on Serverless replications, and select the replication that you created one at a time if you created multiple.\nClick on the Actions button on the right-hand side, and then select Stop.\nConfirm that you want to stop the replication.\nAfter the status of the replication changes to Stopped, click on the Actions button again, and then select Delete.\nConfirm that you want to delete the replication.\n"
},
{
	"uri": "/6-troubleshooting/6.1-memorypressure/6.1.1-environmentcreation/",
	"title": "Environment Creation",
	"tags": [],
	"description": "",
	"content": "As we mentioned earlier, memory pressure on DMS instance may happen when migration task put high load on the DMS instance and the allocated memory is not enough to handle the workload. Hence to recreate this scenario, we will put load via DMS task(s).\nIn this troubleshooting exercise, we are going to use resources (DMS task/RDS instances etc) created under Oracle Source to Amazon Aurora (PostgreSQL) Target lab. However, if you are following any other lab with any other RDBMS source/target engine(s), feel free to continue using those resources.\nNavigate to list of DMS task on AWS DMS console. Select the DMS full load migration task. From Actions dropdown menu, click on Stop.\nAfter the task status changes to Stopped, once again, open Actions \u0026gt; Modify.\nOn the Modify data migration task page, scroll down to Advanced task settings section. Under Full load tuning settings, put 49 in text box for Maximum number of tables to load in parallel and 50000 in Commit rate during full load text box.\nNow scroll down to Table mappings section. Under Selection Rules, replace Schema name from DMS_SAMPLE to %. Now scroll down to bottom of the page and hit the Save button.\nOnce task modification finish, go ahead and RESTART (not resume) the task.\nMonitor DMS instance cloud watch metrics. When load on DMS instance memory increases, you may notice decrease in Freeable Memory and increase/fluctuations in SwapUsage. Also monitor DMS task status. After sometime, you will notice that DMS task status is changed to Failed.\nNavigate to failed DMS task, under Overview details tab, notice the Last failure message. If DMS task failed due to memory pressure on DMS instance, you will see the message \u0026ldquo;Last Error Replication task out of memory. Stop Reason FATAL_ERROR Error Level FATAL.\u0026rdquo;\nAt this stage, we are able to successfully create the scenario where DMS task fails due to memory pressure on DMS instance!\nIf you notice task successfully completed even after adding more tables on task, you may add even more tables on the task. Alternatively you may also create separate task on same instance and run all tasks in parallel.\n"
},
{
	"uri": "/6-troubleshooting/6.2-tableerrors/6.2.1-environmentcreation/",
	"title": "Environment Creation",
	"tags": [],
	"description": "",
	"content": "In this troubleshooting exercise, we are going to use resources (DMS task/RDS instances etc) created under Oracle Source to Target Amazon Aurora (PostgreSQL) lab. We assuming that you have already created DMS task using this lab and one time migration also completed.\nIn this troubleshooting exercise, we are going to use resources (DMS task/RDS instances etc) created under Oracle Source to Target Amazon Aurora (PostgreSQL) lab. However, if you are following any other lab with any other RDBMS target engine(s), feel free to continue using those resources.\nNow, go ahead and RESTART the same DMS task oracle-migration-task again.\nAfter this task is running for couple of minutes, you will notice the task went into error state. You may observe task status being either Running with errors or Error.\n"
},
{
	"uri": "/4-serverless/4.2-scaletesting/4.2.1-generateload/",
	"title": "Generate Load on the Source Database",
	"tags": [],
	"description": "",
	"content": "Stop and Modify DMS Serverless Task Before generating load on the Source database, first stop the currently running DMS serverless data migration task in order for us to change some settings of DMS Serverless task\nStop the running task:\nClick on Actions, and then select Stop.\nModify the task settings:\nOnce the task status changes from \u0026ldquo;Stopping\u0026rdquo; to \u0026ldquo;Stopped\u0026rdquo;, select the task again and click on the Modify button to change the memory, max DCU and CW Logs settings\n2.1. Update memory settings to handle increased load:\nTo ensure the replication task can manage the increased load, update the memory settings. Without these updates, you may encounter the \u0026ldquo;Reading from source paused\u0026rdquo; error.\n2.2. Switch to JSON View under Settings. Look for the **ChangeProcessingTuning** tag and change the following:\nMemoryKeepTime: Set to 600 (fron 60).\nMemoryLimitTotal: Set to 10240 (from 1024).\nIf you do not change these settings, the DMS task will throttle when MemoryLimitTotal is reached and you may not see scaling operation. These settings ensure that the task can handle increased load efficiently. Refer to Change processing tuning settings for more information on these parameters\n2.3. Remove CloudWatch log tags:\nDelete the \u0026ldquo;CloudWatchLogGroup\u0026rdquo; and \u0026ldquo;CloudWatchLogStream\u0026rdquo; tags.\nBefore: After: 2.4. Increase Max DMS capacity units so that the task can scale up when load on source server increases:\nUnder Compute Settings, change the Maximum DMS Capacity Units (DCU) to 16. Save changes and resume the task:\nResume the task once the changes are saved and status changes from \u0026ldquo;Modifying\u0026rdquo; to \u0026ldquo;Stopped\u0026rdquo; state. Generate Load on Source database Lets simulate a bustling social media site by creating 3 tables and a stored procedure to generate thousands of posts, users, and votes at random intervals, mimicking the unpredictable traffic of a live social media platform.\nNote: expand only for the source in your scenario. You can skip other sources.\nFor SQL Server as Source: Run SQL load script:\nOpen a new remote desktop session to your bastion host and connect to SQL Server source database using SQL Server Management Studio (SSMS). Once connected, click on New Query in SSMS.\nIn the query window, copy and paste the contents of the provided mssql_load_script.sql and click Execute.\nThis will create three tables: Post, Users, and Votes, along with the necessary data generation stored procedures. Simulate heavy load on the source database:\nRun the following code directly in the command prompt to start 10 parallel sessions: for /L %i in (1,1,10) do start cmd /k sqlcmd -S localhost -U \u0026lt;\u0026lt;Enter Username\u0026gt;\u0026gt; -P \u0026lt;\u0026lt;Enter Password\u0026gt;\u0026gt; -Q \u0026#34;USE dms_sample; EXEC usp_RandomQ 100000;\u0026#34; This will open 10 simultaneous sessions, each generating load on your source SQL Server database. For Oracle as Source: Run SQL load script:\nOpen a new remote desktop session to your bastion host and connect to your Oracle database using SQL Developer.\nClick on the SQL Worksheet icon within Oracle SQL Developer, then connect to the Source Oracle database.\nIn the query window, copy and paste the contents provided from oracle_load_script.sql and click Execute.\nThis will create three tables: Post, Users, and Votes, along with the necessary data generation stored procedures.\nSimulate heavy load on the source database:\nRun the following code directly in a new SQL Worksheet window to generate load on your source Oracle database: EXEC usp_RandomQ(100000); We have generated the load, and now we will monitor the CPU utilization as it approaches the 80% threshold, which will trigger a scaling event to increase the current DCUs from 4 to 8. This process may take approximately 20 to 30 minutes. Please proceed to the next section to learn how to monitor the data migration task and observe the scaling event.\n"
},
{
	"uri": "/1-start/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;re using your own AWS account, or you are advised by AWS staff to configure the workshop environment, you need to setup some resources on AWS that we will use to perfrom the migration. This section describes the steps to provision the AWS resources that are required for this database migration walkthrough.\nWe use AWS CloudFormation to simplify the provisioning of the infrastructure, so we can concentrate on tasks related to data migration.\nThe CFN Template creates a basic network topology that includes Amazon Virtual Private Cloud (Amazon VPC) with 3 public subnets to deploy the AWS Database Migration Service (AWS DMS) replicaiton instance, as well as Amazon Relational Database Service (Amazon RDS) instance for the target database. Additionally, it provisions an Amazon Elastic Compute Cloud (EC2) instance to host the tools that we use in this migration, including the AWS Schema Conversion Tool (AWS SCT). Likewise, in the Microsoft SQL Server migration workshop, we use this EC2 instance to simulate our source database.\n"
},
{
	"uri": "/3-migratetotarget/3.1-schemaconv/3.1.1-configmysql/",
	"title": "Granting permissions in the target database",
	"tags": [],
	"description": "",
	"content": "\rThis section is only for the Aurora (MySQL compatible) target. If you are migrating to a different target, you can skip this page. \\ Aurora MySQL will need some additional permissions before we run SCT, so make your you did the following steps before proceeding to the next section. Expand to see this section if your migration target schema is Aurora MySQL\rOpen MySQL Workbench 8.0 CE from within the EC2 server, and create a new database connection for the target Aurora database using the following values. Please note the password is not provided below you need to go to Secrets Manager and open DMSDBSecret and reveal the SQL Server password. It is also on first Cloudformation Stack\u0026rsquo;s output tab under SQL Server password (yes this is mysql but the passwords are the same for sql server \u0026amp; mysql in this case):\nParameter Value Connection Name Target Aurora RDS (MySQL) Host Name - you can find this on Cloudformation \u0026gt; cfn \u0026gt; output tab or you can goto RDS in the AWS console and find your Aurora MySQL Target instance Port 3306 Username dbadmin Password (Enter after clicking Test Connection) See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for SQL Server Default Schema leave blank Click Test Connection button and enter the password. After you receive a message stating “Successfully made the MySQL connection”, click OK.\nClick on the Target Aurora RDS (MySQL) from the list of MySQL Connections in SQL Workbench to connect to the target database.\nCopy the following permission sql statements and execute them in the workbench tool (Lighting icon is execute/run icon).\nGRANT CREATE ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT ALTER ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT DROP ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT INDEX ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT REFERENCES ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT SELECT ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT CREATE VIEW ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT SHOW VIEW ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT TRIGGER ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT CREATE ROUTINE ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT ALTER ROUTINE ON *.* TO \u0026#39;dbadmin\u0026#39;; GRANT EXECUTE ON *.* TO \u0026#39;dbadmin\u0026#39;; -- only for MySQL \u0026lt;8.0 GRANT SELECT ON mysql.proc TO \u0026#39;awssct\u0026#39;; GRANT INSERT, UPDATE ON AWS_SQLSERVER_EXT.* TO \u0026#39;dbadmin\u0026#39;; GRANT INSERT, UPDATE, DELETE ON AWS_SQLSERVER_EXT_DATA.* TO \u0026#39;dbadmin\u0026#39;; GRANT CREATE TEMPORARY TABLES ON AWS_SQLSERVER_EXT_DATA.* TO \u0026#39;dbadmin\u0026#39;; GRANT CREATE TEMPORARY TABLES ON *.* TO \u0026#39;dbadmin\u0026#39;; -- MySQL \u0026lt;8 code GRANT INVOKE LAMBDA ON *.* TO \u0026#39;awssct\u0026#39;; GRANT AWS_LAMBDA_ACCESS TO \u0026#39;dbadmin\u0026#39;; GRANT INSERT, UPDATE ON AWS_ORACLE_EXT.* TO \u0026#39;dbadmin\u0026#39;; GRANT INSERT, UPDATE, DELETE ON AWS_ORACLE_EXT_DATA.* TO \u0026#39;dbadmin\u0026#39;; grant INSERT ON aws_sqlserver_ext.* to \u0026#39;dbadmin\u0026#39;; grant UPDATE ON aws_sqlserver_ext.* to \u0026#39;dbadmin\u0026#39;; After execution, the output should look something like the image below. We suggest you keep MySQL workbench open as you\u0026rsquo;ll come back to it later in the DMS section.\nYou can now change the EC2 instance you are RDP-ed into back to the Schema conversion tool and click Next below.\n"
},
{
	"uri": "/6-troubleshooting/6.1-memorypressure/",
	"title": "Memory Pressure on DMS Instance",
	"tags": [],
	"description": "",
	"content": "In this lab we will troubleshoot DMS task failure due to memory pressure on replication instance.\nBefore we go into creating this scenario, let us recap on how DMS internally works.\nTo perform a database migration, AWS DMS connects to the source data store, reads the source data, and formats the data for consumption by the target data store. It then loads the data into the target data store. Most of this processing happens in DMS replication instance’s memory. Though large transactions might require some buffering to disk. Cached transactions and log files are also written to disk.\nHaving said that, if a migration put too much load on DMS replication instance, in some scenario it can create memory pressure on the instance and can result in task failures. If you run more than one DMS task, these failures can be seen in multiple tasks, if not on all.\nLet\u0026rsquo;s go ahead and create the scenario in next section.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/a-rdsmssql/",
	"title": "Microsoft SQL Server target",
	"tags": [],
	"description": "",
	"content": "In this page, you will inspect the target database after migration.\nOpen Microsoft SQL Server Management Studio from within the RDP-ed onto EC2 server.\nFollow the instructions described earlier to connect to the Target RDS SQL Server instance.\nInspect the migrated RDS SQL database (dms_sample_target) content, and make sure the following tables have migrated over. You can do that by opening a New Query window to execute the following statement.\nuse dms_sample_target SELECT * FROM dms_sample_target.INFORMATION_SCHEMA.TABLES; GO For Oracle Source, this should show 15 dms_sample base tables and 1 DMS related apply exceptions table for a total of 16 tables created\nFor SQL Server Source, this should show 16 dms_sample base tables and 1 DMS related apply exceptions table for a total of 17 tables created\nNext, execute the following query:\nuse dms_sample_target SELECT * FROM dms_sample_target.sport_type; GO Baseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\n"
},
{
	"uri": "/4-serverless/4.1-migrate/",
	"title": "Migrate Data using DMS Serverless",
	"tags": [],
	"description": "",
	"content": "This step-by-step guide demonstrates how you can use AWS Database Migration Service (DMS) to migrate data from SQL Server or Oracle database running on an Amazon EC2. Additionally, you will configure AWS DMS to capture data changes (CDC) on the source database and replicate them on the target database.\nPrerequisites Setting up your AWS Console, Download EC2 Key Pair and Retrieve Environment parameters. (Getting Started)\nConfigure your source database by selecting one of the following links:\nSQL Server as source\nOracle as source\nCreate Source and Target endpoint creation:\nAurora MySQL as Target\nAurora PostgreSQL as Target\nOnce you have configured your source database and created source and target endpoints. You can proceed to create a Serverless Replication task.\n"
},
{
	"uri": "/3-migratetotarget/3.1-schemaconv/3.1.3-schemaconv/modifycode/",
	"title": "Modify the Procedural Code",
	"tags": [],
	"description": "",
	"content": "In this part, we support modifying the procedural code for SQL Server that Schema Conversion Tool could not automatically convert for the target database dialect.\nThe first three steps is for SQL Server source only. If you use the Oracle source, you can skip to step 4.\nFrom the left panel, uncheck the items with the exclamation mark except for the generateTransferActivity procedure.\nNext, click on the generateTransferActivity procedure. Observe how the SCT highlights the issue, stating that MySQL does not support the PRINT procedure. To fix this, you need to replace the three highlighted PRINT statements with SELECT statement as demonstrated in the following example:\nNote: suggest you use \u0026ndash; in front of current PRINT line to comment it out then copy and paste the correct syntax for the first 2 occurences then copy the last correct syntax for the last one. You\u0026rsquo;ll need to do this all in 1 edit session so arrow down after going to the firt line\nMS SQL Server syntax:\nPRINT (concat(\u0026#39;max t: \u0026#39;,@max_tik_id,\u0026#39; min t: \u0026#39;, @min_tik_id, \u0026#39;max p: \u0026#39;,@max_p_id,\u0026#39; min p: \u0026#39;, @min_p_id)); MySQL syntax:\n--first 2 use line below SELECT concat(\u0026#39;max t: \u0026#39;,@max_tik_id,\u0026#39; min t: \u0026#39;, @min_tik_id, \u0026#39;max p: \u0026#39;,@max_p_id,\u0026#39; min p: \u0026#39;, @min_p_id); --third one use SELECT (\u0026#39;Sorry, no tickets are available for transfer.\u0026#39;); After you make the modification, right-click on the dbo schema, and choose Create Report. Observe that the schema of the source database is now fully compatible with the target database.\nNote: For non-SQL Server sources (e.g. Oracle), it says step 4. However, this is really your first step on this page. Thanks for your understanding!!\nRight click on the dms_sample database in the left panel and select Convert Schema to generate the data definition language (DDL) statements for the target database.\nWhen warned that objects may already exist in database, click Yes.\nYou may be presented with a different message stating “The operation will be partially performed” and only 3 of the 8 objects will be converted. Click OK.\nRight click on the dms_sample \u0026gt; dbo (SQL Server source) or dms_sample (other sources) schema in the right-hand panel, and click Apply to database.\nWhen prompted if you want to apply the schema to the database, click Yes.\nAt this point, the schema has been applied to the target database. Expand the dms_sample_dbo or dms_sample schema on the right hand panel to see the tables, views, store procedures etc.\nYou have sucessfully converted the database schema and object from source to the format compatible with Amazon Aurora (MySQL).\nThis part demonstrated how easy it is to migrate the schema of a source database into Amazon Aurora (MySQL) using the AWS Schema Conversion Tool. Similarly, you learned how the Schema Conversion Tool highlights the differences between different database engine dialects, and provides you with tips on how you can successfully modify the code when needed to migrate procedure and other database objects.\nThe same steps can be followed to migrate SQL Server and Oracle workloads to other RDS engines including PostgreSQL and MySQL.\nThe next section describes the steps required to move the actual data using AWS DMS. Read on to learn more\u0026hellip;\n"
},
{
	"uri": "/2-selectsource/2.2-sqlsrv/2.2.1-connectsrcdb/",
	"title": "Open SQL Server Management Studio",
	"tags": [],
	"description": "",
	"content": " Once connected to the EC2 instance Remote Desktop session, open SQL Server Management Studio from the Taskbar.\nUse the following values to connect to your source database.\nParameter Value Server Type Database Engine Server Name localhost Authorization Windows Authentication After the you see the test status as Successful, click Connect. Your window should look like below.\n"
},
{
	"uri": "/2-selectsource/2.1-oracle/",
	"title": "Oracle source",
	"tags": [],
	"description": "",
	"content": "Now that you have completed setting up the Getting Started, you are ready to migrate a sample data base.\nThis step-by-step guide demonstrates how you can use AWS Database Migration Service (DMS) to migrate data from an Oracle database which in this case is running on an Amazon RDS but it is the same process as Oracle running on prem or anywhere else. Additionally, you will configure AWS DMS to capture data changes (CDC) on the source database and replicate them on the target database.\nThe environment for this lab consists of:\nAn EC2 instance that hosts the tools used in this lab such as Oracle SQL Developer.\nAn Amazon RDS Oracle instance used to host the source Oracle database.\nA DMS target\n"
},
{
	"uri": "/3-migratetotarget/3.1-schemaconv/",
	"title": "Schema Conversion",
	"tags": [],
	"description": "",
	"content": "\rThis section is only apply for Amazon Aurora targets. If you are using another target, you can skip to section 3.1.2 \\ Several pictures are taken from the Microsoft SQL to Aurora (PostgreSQL) schema conversion. However, different sources and targets employs similar processes. Thus, this guide should also apply for them. This section demonstrates how to use the AWS Schema Conversion Tool for schema conversion from Microsoft SQL Server or Oracle to Amazon Aurora (MySQL or PostGreSQL). Additionally, you will observe how AWS SCT helps you spot the differences between the two dialects; and, provides you with tips about how you can modify procedural code when needed to successfully migrate all database objects.\n"
},
{
	"uri": "/1-start/1.1-signin/",
	"title": "Sign in to your AWS Account",
	"tags": [],
	"description": "",
	"content": " Login to AWS Management Console using your credentials.\nClick on the drop-down menu on the top right corner of the screen, and select one of the 20 supported regions for this tutorial:\nRegion Name Region Code US East (N. Virginia) us-east-1 US East (Ohio) us-east-2 US West (Oregon) us-west-2 Canada (Central) ca-central-1 EU (Frankfurt) eu-central-1 EU (Ireland) eu-west-1 EU (London) eu-west-2 EU (Paris) eu-west-3 EU (Milan) eu-south-1 EU (Stockholm) eu-north-1 Asia Pacific (Hong Kong) ap-east-1 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Tokyo) ap-northeast-1 Asia Pacific (Osaka) ap-northeast-3 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 South America (Sao Paulo) sa-east-1 Africa (Cape Town) af-south-1 Middle East (Bahrain) me-south-1 "
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.1-configtarget/b-auroramysql/",
	"title": "Aurora (MySQL compatible) target",
	"tags": [],
	"description": "",
	"content": " Open MySQL Workbench 8.0 CE used earlier to add permissions needed for Aurora MySQL as a DMS target.\nIn this step you are going to drop the foreign key constraint from the target database:\nFrom within MySQL Workbench, open a query window and copy and paste the code below into the query window and execute.\nNote: expand only for the source in your scenario you can skip the other sources.\nExpand if you have an Oracle Source\r/* SCT doesn\u0026#39;t create the constraints from oracle automatically there only indexes to drop */ drop index set_seat_idx on dms_sample.sporting_event_ticket; drop index set_ticketholder_idx on dms_sample.sporting_event_ticket; drop index set_sporting_event_idx on dms_sample.sporting_event_ticket; Expand if you have a Microsoft SQL Server Source\rALTER TABLE dms_sample_dbo.player DROP FOREIGN KEY sport_team_fk; ALTER TABLE dms_sample_dbo.seat DROP FOREIGN KEY seat_type_fk; ALTER TABLE dms_sample_dbo.seat DROP FOREIGN KEY s_sport_location_fk; ALTER TABLE dms_sample_dbo.sport_division DROP FOREIGN KEY sd_sport_type_fk; ALTER TABLE dms_sample_dbo.sport_division DROP FOREIGN KEY sd_sport_league_fk; ALTER TABLE dms_sample_dbo.sport_league DROP FOREIGN KEY sl_sport_type_fk; ALTER TABLE dms_sample_dbo.sport_team DROP FOREIGN KEY st_sport_type_fk; ALTER TABLE dms_sample_dbo.sport_team DROP FOREIGN KEY home_field_fk; ALTER TABLE dms_sample_dbo.sporting_event DROP FOREIGN KEY se_sport_type_fk; ALTER TABLE dms_sample_dbo.sporting_event DROP FOREIGN KEY se_away_team_id_fk; ALTER TABLE dms_sample_dbo.sporting_event DROP FOREIGN KEY se_home_team_id_fk; ALTER TABLE dms_sample_dbo.sporting_event DROP FOREIGN KEY se_location_id_fk; ALTER TABLE dms_sample_dbo.sporting_event_ticket DROP FOREIGN KEY set_person_id; ALTER TABLE dms_sample_dbo.sporting_event_ticket DROP FOREIGN KEY set_sporting_event_fk; ALTER TABLE dms_sample_dbo.sporting_event_ticket DROP FOREIGN KEY set_seat_fk; ALTER TABLE dms_sample_dbo.ticket_purchase_hist DROP FOREIGN KEY tph_sport_event_tic_id; ALTER TABLE dms_sample_dbo.ticket_purchase_hist DROP FOREIGN KEY tph_ticketholder_id; ALTER TABLE dms_sample_dbo.ticket_purchase_hist DROP FOREIGN KEY tph_transfer_from_id; drop index set_ev_id_tkholder_id_idx on dms_sample_dbo.sporting_event_ticket; drop index set_seat_idx on dms_sample_dbo.sporting_event_ticket; drop index set_ticketholder_idx on dms_sample_dbo.sporting_event_ticket; drop index set_sporting_event_idx on dms_sample_dbo.sporting_event_ticket; Execute the script (click the bolt icon). the results should look similar to the window below.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/b-auroramysql/",
	"title": "Aurora (MySQL compatible) target",
	"tags": [],
	"description": "",
	"content": "In this page, you will inspect the target database after migration.\nOpen MySQL Workbench 8.0 CE from within the EC2 server, and click on Target Aurora RDS (MySQL) database connection that you created earlier.\nInspect the migrated data, by querying one of the tables in the target database. For example, the following query should return a table with two rows:\nNote: expand only for the source in your scenario you can skip the other sources.\nFor Oracle Source expand this\rSELECT * FROM dms_sample.sport_type; For SQL Server Source expand this\rSELECT * FROM dms_sample_dbo.sport_type; Baseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\nBefore running the next script block we will need to change preferences in MySQL Workbench. Go to Edit from menu then Preferences then SQL Editor and increase DBMS Connection read and timeout settings to 600 vs. defaults like below\nNow, disconnect your MySql workbench \u0026amp; reconnect to ensure the preference changes take effect. To do this click the X on the MySQL Target tab besides the home icon upper left then double click on the MySQL Target connection box to reconnect\nUse the following script that matches your source database selected to enable the foreign key constraints that we dropped earlier. Within MySQL Workbench, copy and paste the sql below and execute it.\nNote: this will take a long time to complete you can move onto the next step while it is running. If errors occur they can be ignored or resolved later.\nFor Oracle Source expand this\rUSE dms_sample; ALTER TABLE dms_sample.player ADD CONSTRAINT sport_team_fk FOREIGN KEY (sport_team_id) REFERENCES dms_sample.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample.seat ADD CONSTRAINT seat_type_fk FOREIGN KEY (seat_type) REFERENCES dms_sample.seat_type (name) ON DELETE CASCADE; /* FK is incompatible in Oracle ALTER TABLE dms_sample.seat ADD CONSTRAINT s_sport_location_fk FOREIGN KEY (sport_location_id) REFERENCES dms_sample.sport_location (id) ON DELETE CASCADE; */ ALTER TABLE dms_sample.sport_division ADD CONSTRAINT sd_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_division ADD CONSTRAINT sd_sport_league_fk FOREIGN KEY (sport_league_short_name) REFERENCES dms_sample.sport_league (short_name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_league ADD CONSTRAINT sl_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name); ALTER TABLE dms_sample.sport_team ADD CONSTRAINT st_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_team ADD CONSTRAINT home_field_fk FOREIGN KEY (home_field_id) REFERENCES dms_sample.sport_location (id) ON DELETE CASCADE; /* table is too big for our lab time to add constraints back ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name); ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_away_team_id_fk FOREIGN KEY (away_team_id) REFERENCES dms_sample.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_home_team_id_fk FOREIGN KEY (home_team_id) REFERENCES dms_sample.sport_team (id); ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_location_id_fk FOREIGN KEY (location_id) REFERENCES dms_sample.sport_location (id); ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_person_id FOREIGN KEY(ticketholder_id) REFERENCES dms_sample.person (ID) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_sporting_event_fk FOREIGN KEY (sporting_event_id) REFERENCES dms_sample.sporting_event (id) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_seat_fk FOREIGN KEY (sport_location_id, seat_level, seat_section, seat_row, seat) REFERENCES dms_sample.seat (sport_location_id, seat_level, seat_section, seat_row, seat); */ ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_sport_event_tic_id FOREIGN KEY (sporting_event_ticket_id) REFERENCES dms_sample.sporting_event_ticket (id) ON DELETE CASCADE; ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_ticketholder_id FOREIGN KEY (purchased_by_id) REFERENCES dms_sample.person (ID); ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_transfer_from_id FOREIGN KEY (transferred_from_id) REFERENCES dms_sample.person (ID); /* --- these will time out in mysql workbench unless you select Edit \u0026gt; Preferences \u0026gt; SQL Editor --- and change DBS timeout settings to all be 600 seconds vs. 30 / 60 --- Note these aren\u0026#39;t required but here for reference since we dropped them for load speed ---create index set_ev_id_tkholder_id_idx on dms_sample_dbo.sporting_event_ticket (sporting_event_id,ticketholder_id); ---create index set_seat_idx on dms_sample_dbo.sporting_event_ticket (sport_location_id,seat_level,seat_section,seat_row,seat); ---create index set_ticketholder_idx on dms_sample_dbo.sporting_event_ticket (ticketholder_id); ---CREATE INDEX set_sporting_event_idx ON dms_sample_dbo.sporting_event_ticket (sporting_event_id); */ For SQL Server Source expand this\rUSE dms_sample_dbo; ALTER TABLE dms_sample_dbo.player ADD CONSTRAINT sport_team_fk FOREIGN KEY (sport_team_id) REFERENCES dms_sample_dbo.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.seat ADD CONSTRAINT seat_type_fk FOREIGN KEY (seat_type) REFERENCES dms_sample_dbo.seat_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.seat ADD CONSTRAINT s_sport_location_fk FOREIGN KEY (sport_location_id) REFERENCES dms_sample_dbo.sport_location (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_division ADD CONSTRAINT sd_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_division ADD CONSTRAINT sd_sport_league_fk FOREIGN KEY (sport_league_short_name) REFERENCES dms_sample_dbo.sport_league (short_name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_league ADD CONSTRAINT sl_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name); ALTER TABLE dms_sample_dbo.sport_team ADD CONSTRAINT st_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_team ADD CONSTRAINT home_field_fk FOREIGN KEY (home_field_id) REFERENCES dms_sample_dbo.sport_location (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name); ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_away_team_id_fk FOREIGN KEY (away_team_id) REFERENCES dms_sample_dbo.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_home_team_id_fk FOREIGN KEY (home_team_id) REFERENCES dms_sample_dbo.sport_team (id); ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_location_id_fk FOREIGN KEY (location_id) REFERENCES dms_sample_dbo.sport_location (id); /* commenting these out as they take too long to perform during our workshop --- these will time out in mysql workbench unless you select Edit \u0026gt; Preferences \u0026gt; SQL Editor --- and change DBS timeout settings to all be 600 seconds vs. 30 / 60 --- Note these aren\u0026#39;t required but here for reference since we dropped them for load speed ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_person_id FOREIGN KEY(ticketholder_id) REFERENCES dms_sample_dbo.person (ID) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_sporting_event_fk FOREIGN KEY (sporting_event_id) REFERENCES dms_sample_dbo.sporting_event (id) ON DELETE CASCADE; */ ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_seat_fk FOREIGN KEY (sport_location_id, seat_level, seat_section, seat_row, seat) REFERENCES dms_sample_dbo.seat (sport_location_id, seat_level, seat_section, seat_row, seat); ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_sport_event_tic_id FOREIGN KEY (sporting_event_ticket_id) REFERENCES dms_sample_dbo.sporting_event_ticket (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_ticketholder_id FOREIGN KEY (purchased_by_id) REFERENCES dms_sample_dbo.person (ID); ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_transfer_from_id FOREIGN KEY (transferred_from_id) REFERENCES dms_sample_dbo.person (ID); /* --- these will time out in mysql workbench unless you select Edit \u0026gt; Preferences \u0026gt; SQL Editor --- and change DBS timeout settings to all be 600 seconds vs. 30 / 60 --- Note these aren\u0026#39;t required but here for reference since we dropped them for load speed ---create index set_ev_id_tkholder_id_idx on dms_sample_dbo.sporting_event_ticket (sporting_event_id,ticketholder_id); ---create index set_seat_idx on dms_sample_dbo.sporting_event_ticket (sport_location_id,seat_level,seat_section,seat_row,seat); ---create index set_ticketholder_idx on dms_sample_dbo.sporting_event_ticket (ticketholder_id); ---CREATE INDEX set_sporting_event_idx ON dms_sample_dbo.sporting_event_ticket (sporting_event_id); */ Execute the script above for the source you selected\nBaseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\n"
},
{
	"uri": "/4-serverless/4.1-migrate/4.1.2-replicationstages/",
	"title": "AWS DMS Serverless Replication Stages",
	"tags": [],
	"description": "",
	"content": "To handle the resources required for data replication, AWS DMS Serverless transitions through different stages that reflect the internal processes of the service. When a replication is initiated, AWS DMS Serverless assesses the workload, allocates the necessary resources, and begins data replication based on the following states.\nInitializing: AWS DMS Serverless kicks off replication by setting up the necessary parameters for the task. Initialisation takes around 2 to 5 mins.\nPreparing Metadata Resources: The system establishes a connection to your source database to gather metadata required for replication. This phase may take around 15 to 20 mins.\nTesting Connection: DMS Serverless checks if connections to both source and target databases are working properly. Testing source and target connection may take upto 2 to 5 mins.\nFetching Metadata: Metadata from the source is retrieved, allowing DMS Serverless to analyze database structure and capacity needs. It may take upto 5 mins to analyze metadata.\nCalculating Capacity: Based on the fetched metadata, DMS estimates the amount of resources required to handle replication tasks. This task may take 1 to 2 mins.\nProvisioning Capacity: AWS DMS allocates the necessary compute resources based on the earlier calculations to support the replication. It might take anywhere between 15 to 20 mins to provision serverless replication.\nReplication Starting: Data replication begins, including phases such as full load and Change Data Capture (CDC), which captures ongoing changes. It takes 1 to 2 mins to start replication process.\nRunning: Continuous replication is now underway, keeping source and target systems in sync. It can transition to a stopped state based on user actions.\n"
},
{
	"uri": "/2-selectsource/2.2-sqlsrv/2.2.2-configsrcdb/",
	"title": "Configure The Source MS SQL Server Database",
	"tags": [],
	"description": "",
	"content": "\rWhen migrating your Microsoft SQL Server database using AWS DMS, you can choose to migrate existing data only, migrate existing data and replicate ongoing changes, or migrate existing data and use Change Data Capture (CDC) to replicate ongoing changes. Migrating only the existing data does not require any configuration on the source SQL Server database. However, to migrate existing data and replicate ongoing changes, you need to either enable MS-REPLICATION, or MS-CDC.\nWith SQL Server Mgmt studio, you are going to execute a sql script that will configure the source sql server database for replication. First ensure there is a query window if not click on the New query icon above the execute button to open a new query window.\nCopy the sql commands below using the copy button top right corner of the code block then paste into SQL Server Management Studio query editor and execute the entire code block (Ctrl + A for windows user to highlight whole block) and click the Execute button.\nWe are going to run a backup on the database plus some other scripts so it will take ~5-12 minutes to run end to end. Once you start the script please proceed to the next step and come back in a few minutes to ensure it finished. You can go to the message tab to monitor progress.\n--Set master database context use [master] GO --Add the awssct login to the sysadmin server role - required for replication ALTER SERVER ROLE [sysadmin] ADD MEMBER [dbadmin] GO --Set the recovery model to full for dms_sample - required for replication ALTER DATABASE [dms_sample] SET RECOVERY FULL WITH NO_WAIT GO BACKUP DATABASE dms_sample TO DISK = \u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\Backup\\dms_sample-fullbkup.bak\u0026#39; WITH FORMAT, COMPRESSION, STATS = 5; GO --Configure this SQL Server as its own distributor exec sp_adddistributor @distributor = @@SERVERNAME, @password = N\u0026#39;Password1\u0026#39; exec sp_adddistributiondb @database = N\u0026#39;distribution\u0026#39;, @data_folder = N\u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\DATA\u0026#39;, @log_folder = N\u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\DATA\u0026#39;, @log_file_size = 2, @min_distretention = 0, @max_distretention = 72, @history_retention = 48, @security_mode = 1 GO RECONFIGURE go --Change context to the distribution database use [distribution] GO --Configure replication if (not exists (select * from sysobjects where name = \u0026#39;UIProperties\u0026#39; and type = \u0026#39;U \u0026#39;)) create table UIProperties(id int) if (exists (select * from ::fn_listextendedproperty(\u0026#39;SnapshotFolder\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;dbo\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;UIProperties\u0026#39;, null, null))) EXEC sp_updateextendedproperty N\u0026#39;SnapshotFolder\u0026#39;, N\u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\repldata\u0026#39;, \u0026#39;user\u0026#39;, dbo, \u0026#39;table\u0026#39;, \u0026#39;UIProperties\u0026#39; else EXEC sp_addextendedproperty N\u0026#39;SnapshotFolder\u0026#39;, N\u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\repldata\u0026#39;, \u0026#39;user\u0026#39;, dbo, \u0026#39;table\u0026#39;, \u0026#39;UIProperties\u0026#39; GO RECONFIGURE go exec sp_adddistpublisher @publisher = @@SERVERNAME, @distribution_db = N\u0026#39;distribution\u0026#39;, @security_mode = 1, @working_directory = N\u0026#39;C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\repldata\u0026#39;, @trusted = N\u0026#39;false\u0026#39;, @thirdparty_flag = 0, @publisher_type = N\u0026#39;MSSQLSERVER\u0026#39; GO /* Oct 2024 temp script additional ****** Temporary Script to create publication and add articles for purpose of Full Load and CDC DMS task to address issue with SQL Server 2022 CU2+ Oct-2024 hopefully by end of 2024 we can remove it INSTRUCTIONS FOR USE 1. Set the session context to the DB you wish to migrate. 2. If you wish to exclude specific user tables that would normally be included, these table names can be added in the field marked by the string \u0026#34;ExplicitlyExcludedTables\u0026#34;. The script contains logic to automatically exclude tables without Primary Keys, system tables and DMS related tables. */ use [dms_sample] go --- Phase 1: Create Publication --- Declare variables DECLARE @db_name NVARCHAR(255); SET @db_name = DB_NAME(); --Set DB for Replication IF not exists ( SELECT 1 FROM sysdatabases d where d.dbid=db_id() and (category \u0026amp; 1)\u0026lt;\u0026gt;0 ) BEGIN EXEC sp_replicationdboption @dbname = @db_name, @optname = \u0026#39;publish\u0026#39;, @value = \u0026#39;true\u0026#39; END ---Declare variables DECLARE @database_id INT; DECLARE @publication_name NVARCHAR(255); DECLARE @publication_description NVARCHAR(MAX); DECLARE @sql_sp_addarticle NVARCHAR(MAX); DECLARE @sql_sp_articlefilter NVARCHAR(MAX); DECLARE @article_name NVARCHAR(255); DECLARE @article_filter_name NVARCHAR(255); DECLARE @schema_name NVARCHAR(128); DECLARE @table_name NVARCHAR(128); DECLARE @object_id NVARCHAR(128); DECLARE @instance_name NVARCHAR(255); SET @instance_name = @@servername; SET @database_id = (SELECT DB_ID()); IF @database_id\u0026lt;10 BEGIN SET @publication_name = CONCAT(\u0026#39;AR_PUBLICATION_0000\u0026#39;,@database_id); SET @article_name = CONCAT(\u0026#39;AR_ARTICLE_0000\u0026#39;,@database_id); SET @article_filter_name = CONCAT(\u0026#39;AR_ARTICLE_0000\u0026#39;,@database_id); END ELSE IF @database_id\u0026gt;=10 and @database_id\u0026lt;100 BEGIN SET @publication_name = CONCAT (\u0026#39;AR_PUBLICATION_000\u0026#39;,@database_id); SET @article_name = CONCAT(\u0026#39;AR_ARTICLE_000\u0026#39;,@database_id); SET @article_filter_name = CONCAT(\u0026#39;AR_ARTICLE_000\u0026#39;,@database_id); END ELSE IF @database_id\u0026gt;=100 and @database_id\u0026lt;1000 BEGIN SET @publication_name = CONCAT (\u0026#39;AR_PUBLICATION_00\u0026#39;,@database_id); SET @article_name = CONCAT(\u0026#39;AR_ARTICLE_00\u0026#39;,@database_id); SET @article_filter_name = CONCAT(\u0026#39;AR_ARTICLE_00\u0026#39;,@database_id); END ELSE IF @database_id\u0026gt;=1000 and @database_id\u0026lt;10000 BEGIN SET @publication_name = CONCAT (\u0026#39;AR_PUBLICATION_0\u0026#39;,@database_id); SET @article_name = CONCAT(\u0026#39;AR_ARTICLE_0\u0026#39;,@database_id); SET @article_filter_name = CONCAT(\u0026#39;AR_ARTICLE_0\u0026#39;,@database_id); END ELSE BEGIN SET @publication_name = CONCAT (\u0026#39;AR_PUBLICATION_\u0026#39;,@database_id); SET @article_name = CONCAT(\u0026#39;AR_ARTICLE_\u0026#39;,@database_id); SET @article_filter_name = CONCAT(\u0026#39;AR_ARTICLE_\u0026#39;,@database_id); END; SET @publication_description = CONCAT(\u0026#39;AWS Schema Conversion Tool DMS Agent: Anonymous Transactional publication for Database: \u0026#39;, @db_name,\u0026#39; from Publisher: \u0026#39;, @instance_name,\u0026#39;.\u0026#39;); if not exists (select name from syspublications where name=@publication_name) exec sp_addpublication @publication = @publication_name, @description = @publication_description, @sync_method = N\u0026#39;native\u0026#39;, @retention = 1, @allow_push = N\u0026#39;true\u0026#39;, @allow_pull = N\u0026#39;true\u0026#39;, @allow_anonymous = N\u0026#39;true\u0026#39;, @enabled_for_internet = N\u0026#39;false\u0026#39;, @snapshot_in_defaultfolder = N\u0026#39;true\u0026#39;, @compress_snapshot = N\u0026#39;false\u0026#39;, @ftp_port = 21, @ftp_login = N\u0026#39;anonymous\u0026#39;, @allow_subscription_copy = N\u0026#39;false\u0026#39;, @add_to_active_directory = N\u0026#39;false\u0026#39;, @repl_freq = N\u0026#39;continuous\u0026#39;, @status = N\u0026#39;active\u0026#39;, @independent_agent = N\u0026#39;true\u0026#39;, @immediate_sync = N\u0026#39;true\u0026#39;, @allow_sync_tran = N\u0026#39;false\u0026#39;, @autogen_sync_procs = N\u0026#39;false\u0026#39;, @allow_queued_tran = N\u0026#39;false\u0026#39;, @allow_dts = N\u0026#39;false\u0026#39;, @replicate_ddl = 1, @allow_initialize_from_backup = N\u0026#39;false\u0026#39;, @enabled_for_p2p = N\u0026#39;false\u0026#39;, @enabled_for_het_sub = N\u0026#39;false\u0026#39;; --- Phase 2 : Add articles to Publication -- Create a Temporary table that has all the tables from sql server and exclude any system related tables pertaining to DMS, as well as user defined exclusions. select SCHEMA_NAME(uid) as TABLE_SCHEMA, name as TABLE_NAME into #entire_list from sys.sysobjects where type in (\u0026#39;U\u0026#39;) and (category=0 or category = 32) and objectproperty(id, \u0026#39;TableHasPrimaryKey\u0026#39;) =1 and objectproperty( id , \u0026#39;IsSystemTable\u0026#39; ) =0 and name not like \u0026#39;awsdms_changes%\u0026#39; and name not like \u0026#39;awsdms_apply%\u0026#39; and name not like \u0026#39;awsdms_truncation%\u0026#39; and name not like \u0026#39;awsdms_cdc_%awsdms_full_load_exceptions%\u0026#39; and name not in (\u0026#39;awsdms_audit_table\u0026#39;,\u0026#39;awsdms_status\u0026#39;,\u0026#39;awsdms_suspended_tables\u0026#39;,\u0026#39;awsdms_history\u0026#39;,\u0026#39;awsdms_validation_failure\u0026#39;) and name not in (\u0026#39;\u0026#39;) --ExplicitlyExcludedTables order by TABLE_SCHEMA,TABLE_NAME --- Create a Temporary table that has all the tables that are already part of the SQL server publication select A.source_owner as TABLE_SCHEMA, A.source_object as TABLE_NAME INTO #All_published FROM distribution.dbo.MsArticles AS A INNER JOIN distribution.dbo.MSpublications AS P ON A.publication_id=P.publication_id and P.publication =@publication_name order by TABLE_SCHEMA,TABLE_NAME --- Find Tables that are in database but not added to the transaction publication select A.TABLE_SCHEMA, A.TABLE_NAME into #filtered_list FROM ( select TABLE_SCHEMA,TABLE_NAME from #entire_list EXCEPT select TABLE_SCHEMA, TABLE_NAME from #All_published ) A --- Declare a cursor to iterate through the objects DECLARE add_article_publication CURSOR FOR select obj.name AS TABLE_NAME, obj.object_id as OBEJCT_ID,sch.name AS SCHEMA_NAME from sys.objects obj, sys.schemas sch where sch.schema_id = obj.schema_id and sch.name in (select TABLE_SCHEMA from #filtered_list) and obj.name in (select TABLE_NAME from #filtered_list) --- Open the cursor and then add the articles to the publication OPEN add_article_publication; FETCH NEXT FROM add_article_publication INTO @table_name, @object_id, @schema_name; WHILE @@FETCH_STATUS = 0 BEGIN SET @sql_sp_addarticle =\u0026#39;exec sys.sp_addarticle @publication = N\u0026#39;\u0026#39;\u0026#39; + @publication_name + \u0026#39;\u0026#39;\u0026#39;, @article = N\u0026#39;\u0026#39;\u0026#39; + @article_name + \u0026#39;_\u0026#39; + @object_id + \u0026#39;\u0026#39;\u0026#39;, @source_owner = N\u0026#39;\u0026#39;\u0026#39; + @schema_name + \u0026#39;\u0026#39;\u0026#39;, @source_object = N\u0026#39;\u0026#39;\u0026#39; + @table_name + \u0026#39;\u0026#39;\u0026#39;, @type = N\u0026#39;\u0026#39;logbased\u0026#39;\u0026#39;, @creation_script = N\u0026#39;\u0026#39;\u0026#39;\u0026#39;, @description = N\u0026#39;\u0026#39;\u0026#39;\u0026#39;, @schema_option = 0x050D3, @pre_creation_cmd = N\u0026#39;\u0026#39;drop\u0026#39;\u0026#39;, @identityrangemanagementoption = N\u0026#39;\u0026#39;none\u0026#39;\u0026#39; ,@status = 16, @vertical_partition = N\u0026#39;\u0026#39;false\u0026#39;\u0026#39;, @ins_cmd = N\u0026#39;\u0026#39;CALL [sp_MSins_\u0026#39; + @table_name + \u0026#39;]\u0026#39;\u0026#39;,@del_cmd = N\u0026#39;\u0026#39;CALL [sp_MSdel_\u0026#39; +@table_name + \u0026#39;]\u0026#39;\u0026#39;,@upd_cmd = N\u0026#39;\u0026#39;MCALL [sp_MSupd_\u0026#39; + @table_name + \u0026#39;]\u0026#39;\u0026#39;,@filter_clause = N\u0026#39;\u0026#39;(1=0)\u0026#39;\u0026#39;\u0026#39; EXEC sp_executesql @sql_sp_addarticle SET @sql_sp_articlefilter = \u0026#39;exec sys.sp_articlefilter @publication = N\u0026#39;\u0026#39;\u0026#39;+ @publication_name + \u0026#39;\u0026#39;\u0026#39; , @article = N\u0026#39;\u0026#39;\u0026#39;+ @article_name + \u0026#39;_\u0026#39; + @object_id + \u0026#39;\u0026#39;\u0026#39; , @filter_name = N\u0026#39;\u0026#39;\u0026#39; + @article_filter_name + \u0026#39;_\u0026#39; + @object_id + \u0026#39;\u0026#39;\u0026#39; , @filter_clause = N\u0026#39;\u0026#39;(1=0)\u0026#39;\u0026#39;, @force_invalidate_snapshot = 1, @force_reinit_subscription = 1 \u0026#39; EXEC sp_executesql @sql_sp_articlefilter FETCH NEXT FROM add_article_publication INTO @table_name, @object_id, @schema_name; END --- Cleanup - Deallocate the cursor and then drop any temporary Tables CLOSE add_article_publication; DEALLOCATE add_article_publication; DROP TABLE #filtered_list DROP TABLE #All_published DROP TABLE #entire_list Execute the script above\nSummary This lab demonstrated steps for setting SQL Server database as a source for Database Migration. The next steps are to select your target and follow the steps to move your SQL Server data to it in that section.\n"
},
{
	"uri": "/2-selectsource/2.1-oracle/2.1.2-configsrcdb/",
	"title": "Configure The Source Oracle Database",
	"tags": [],
	"description": "",
	"content": "To use Oracle as a source for AWS Database Migration Service (AWS DMS), you must first provide a DMS user account with read and write privileges on the Oracle database.\nYou also need to ensure that ArchiveLog Mode is on to provide information to LogMiner. AWS DMS uses LogMiner to read information from the archive logs so that AWS DMS can capture changes.\nFor AWS DMS to read this information, make sure the archive logs are retained on the database server as long as AWS DMS requires them. Retaining archive logs for 24 hours is usually sufficient.\nTo capture change data, AWS DMS requires database-level supplemental logging to be enabled on your source database. Doing this ensures that the LogMiner has the minimal information to support various table structures such as clustered and index-organized tables.\nSimilarly, you need to enable table-level supplemental logging for each table that you want to migrate.\nClick on the SQL Worksheet icon within Oracle SQL Developer, then connect to the Source Oracle database.\nNext, copy, paste and execute the below statements to grant the following privileges to the AWS DMS user to access the source Oracle endpoint:\nGRANT CREATE SESSION to DMS_USER; GRANT SELECT ANY TABLE to DMS_USER; GRANT SELECT ANY TRANSACTION to DMS_USER; GRANT SELECT on DBA_TABLESPACES to DMS_USER; --GRANT SELECT ON any-replicated-table to DMS_USER; ---has issues GRANT EXECUTE on rdsadmin.rdsadmin_util to DMS_USER; -- For Oracle 12c only: GRANT LOGMINING to DMS_USER; exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_VIEWS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_TAB_PARTITIONS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_INDEXES\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_OBJECTS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_TABLES\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_USERS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_CATALOG\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_CONSTRAINTS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_CONS_COLUMNS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_TAB_COLS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_IND_COLUMNS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_LOG_GROUPS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$ARCHIVED_LOG\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$LOG\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$LOGFILE\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$DATABASE\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$THREAD\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$PARAMETER\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$NLS_PARAMETERS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$TIMEZONE_NAMES\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$TRANSACTION\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$CONTAINERS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;DBA_REGISTRY\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;OBJ$\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ALL_ENCRYPTED_COLUMNS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$LOGMNR_LOGS\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$LOGMNR_CONTENTS\u0026#39;,\u0026#39;DMS_USER\u0026#39;,\u0026#39;SELECT\u0026#39;); exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;DBMS_LOGMNR\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;EXECUTE\u0026#39;); -- (as of Oracle versions 12.1 and later) exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;REGISTRY$SQLPATCH\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); -- (for Amazon RDS Active Dataguard Standby (ADG)) exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;V_$STANDBY_LOG\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); -- (for transparent data encryption (TDE)) exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;ENC$\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;SELECT\u0026#39;); -- (for validation with LOB columns) exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;DBMS_CRYPTO\u0026#39;, \u0026#39;DMS_USER\u0026#39;, \u0026#39;EXECUTE\u0026#39;); -- (for binary reader) exec rdsadmin.rdsadmin_util.grant_sys_object(\u0026#39;DBA_DIRECTORIES\u0026#39;,\u0026#39;DMS_USER\u0026#39;,\u0026#39;SELECT\u0026#39;); ---archive log \u0026amp; supplement log for cdc exec rdsadmin.rdsadmin_util.set_configuration(\u0026#39;archivelog retention hours\u0026#39;,24); exec rdsadmin.rdsadmin_util.alter_supplemental_logging(\u0026#39;ADD\u0026#39;); exec rdsadmin.rdsadmin_util.alter_supplemental_logging(\u0026#39;ADD\u0026#39;,\u0026#39;PRIMARY KEY\u0026#39;); alter table dms_sample.nfl_stadium_data add supplemental log data (ALL) columns; alter table dms_sample.mlb_data add supplemental log data (ALL) columns; alter table dms_sample.nfl_data add supplemental log data (ALL) columns; Summary This lab demonstrated steps for setting Oracle database as a source for Database Migration. The next steps are to select your target and follow the steps to move your Oracle data to it.\n"
},
{
	"uri": "/1-start/1.2-keypair/",
	"title": "Create a Key Pair",
	"tags": [],
	"description": "",
	"content": "In this step, you will generate an EC2 key pair that you will use to connect to the EC2 instance.\nNavigate to the Key Pair section in the EC2 console. Ensure you are in the same region as you chose in the previous step. Then, click on the Create Key Pair button.\nName the key pair DMSKeyPair (or any name you wish to give it), and then click Create. At this point, your browser will download a file named DMSKeyPair.pem. Save this file. You will need it to complete the tutorial.\nRemember the location that you save DMSKeyPair.pem on your computer. You will use this file later.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.2-repinstance/",
	"title": "Create a replication instance",
	"tags": [],
	"description": "",
	"content": "The following illustration shows a high-level view of the migration process.\nNavigate to the Database Migration Service (DMS) console or type DMS in the search bar top left part of screen.\nOn the left-hand menu click on Replication Instances. This will launch the Replication instance screen.\nNotice we\u0026rsquo;ve already created a DMS Replication instance for you so you can either skip this section or if you want to get the experience of building a DMS replication instance please proceed.\nClick on the Create replication instance button on the top right side.\nEnter the following information for the Replication Instance. Then, click on the Create button.\nParameter Value Name DMSReplication-myinitials Descriptive Amazon Resource name(ARN) leave blank Description Optional or Replication server for Database Migration Instance Class **dms.c5.xlarge** Engine version Leave the default value High Availability/Multi-AZ Dev or Test workload(Single-AZ) Allocated storage (GB) 50 VPC VPC ID with DMSVpc in the name - need to change the drop down Publicly accessible No/unchecked Advanced -\u0026gt; VPC Security Group(s) default Note 1: If you get an error \u0026ldquo;The IAM Role arn aws iam ########## role dms-vpc-role is not configured properly\u0026rdquo;, click Cancel and repeat steps above and it will work on the second attempt. Note 2: Creating replication instance will take several minutes. While waiting for this replication instance to be created, we will go ahead and use the one that was created for you as part of the environment build that is already available.\n"
},
{
	"uri": "/3-migratetotarget/3.1-schemaconv/3.1.2-createproject/",
	"title": "Create a Schema Conversion Project",
	"tags": [],
	"description": "",
	"content": "Now that you have installed the AWS Schema Conversion Tool, the next step is to create a Database Migration Project using the tool.\nSpecify the source database Within the Schema Conversion Tool, if the new project wizard doesn\u0026rsquo;t start please start it by going to File \u0026gt; New Project Wizard top left corner. Once in Project wizard mode, please enter the following values into the form note you\u0026rsquo;ll need to adjust for the source database you selected in the text below and then click Next.\nParameter Value Project Name AWS Schema Conversion Tool Source DB to Aurora MySQL (or Aurora PostgresSQL) Location C:\\Users\\Administrator\\AWS Schema Conversion Tool Type Radio Button Click/Select SQL Database Source Database Engine Microsoft SQL Server, Oracle or Source you selected earlier in this session \u0026ldquo;I want to\u0026rdquo; Radio Button Click/Select I want to switch engines and optimize for the cloud Specify the source database configurations in the form, Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the SQLServer password value. It is also on first Cloudformation Stack\u0026rsquo;s output tab (SQLServerPassword) and click Test Connection. Once the connection is successfully tested, click Next.\nDepending on your source database, choose one of the following routes to follow the steps.\nOracle Source Information Expand to see\rParameter Value Connection Name (if there) Oracle Source Type SID Server Name \u0026lt; SourceOracleEndpoint (you can find this in Cloudformation Output tab or goto RDS service in Server Port 1521 Oracle SID ORACLEDB User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for Oracle Use SSL Unchecked Store Password Checked Oracle Driver Path C:\\Users\\Administrator\\Desktop\\DMS Workshop\\JDBC\\ojdbc11.jar MS SQL Server Source Information Expand to see\rParameter Value Connection Name (if there) SQL Server Source Project Name localhost Server Port 1433 Instance Name localhost Authentication SQL Server Authentication User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretPSQLSERVER or look it up in AWS Secrets Manager for SQL Server Use SSL Unchecked Store Password Checked Microsoft SQL Server Driver Path C:\\Users\\Administrator\\Desktop\\DMS Workshop\\JDBC\\mssql-jdbc-12.6.1.jre11.jar Select the dms_sample database, then click Next. (Note: once you click on database gray/blue bar highlight should appear \u0026amp; Next button will be enabled)\nReview the Database Migration Assessment Report. SCT will examine in detail all of the objects in the schema of source database. It will convert as much as possible automatically and provides detailed information about items it could not convert. If you scroll down on the report you\u0026rsquo;ll see specific target databases like below for SQL Server to Aurora MySQL.\nGenerally, packages, procedures, and functions are more likely to have some issues to resolve because they contain the most custom or proprietary SQL code. AWS SCT specifies how much manual change is needed to convert each object type. It also provides hints about how to adapt these objects to the target schema successfully.\nAfter you are done reviewing the database migration assessment report, click Next.\nSpecify the target database Specify the target database configurations in the form, Please note the password is not provided below you need to go to Secrets Manager and open DMSDBSecret and reveal the password. It is also on first Cloudformation Stack\u0026rsquo;s output tab see the SQLSever password (same as above) and then click Test Connection. Once the connection is successfully tested, click Finish.\nExpand if you are convert to Amazon Aurora (MySQL compatible)\rParameter Value Target Database Engine Amazon Aurora (MySQL compatible) not the default Connection Name (if there) Aurora MySQL Target Server Name TargetAuroraMySQLEndpoint - you can find this on Cloudformation \u0026gt; cfn \u0026gt; output tab or you can goto RDS in the AWS console and find your MySQLTarget instance Server Port 3306 User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager. Use SSL Unchecked Store Password Checked MySQL Driver Path C:\\Users\\Administrator\\Desktop\\DMS Workshop\\JDBC\\mysql-connector-j-8.3.0.jar Expand if you are convert to Amazon Aurora (PostgreSQL compatible)\rParameter Value Target Database Engine Amazon Aurora (PostgreSQL compatible) not the default Connection Name (if there) Postgresql Target Server Name TargetAuroraPostgreSQLEndpoint - you can find this on Cloudformation \u0026gt; cfn \u0026gt; output tab or you can goto RDS in the AWS console and find your Postgresql Target instance Server Port **5432** Database Name AuroraDB case sensitive User Name **dbadmin** Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for Postgresql Use SSL Unchecked Store Password Checked Amazon Aurora Driver Path **C:\\Users\\Administrator\\Desktop\\DMS Workshop\\JDBC\\postgresql-42.7.3.jar** If copy and paste isn\u0026rsquo;t working use the browse button to the DMS Workshop JDBC folder on the desktop After hitting Next and loading metadata, you may get a warning message saying: Metadata loading was intrupted because of data fetching issues. You can ignore this message as it doesn\u0026rsquo;t affect us in this workshop. Note it will take a few minutes for SCT to analyze the database objects\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/",
	"title": "Data Migration",
	"tags": [],
	"description": "",
	"content": "This section will demonstrate how you can use the AWS Database Migration Service to migrate data from the source database to the target Amazon Aurora (MySQL). Additionally, you will use AWS DMS to continually replicate database changes from the source database to the target database.\nAWS DMS doesn\u0026rsquo;t migrate your secondary indexes, sequences, default values, stored procedures, triggers, synonyms, views, and other schema objects that aren\u0026rsquo;t specifically related to data migration. To migrate these objects to your Aurora MySQL target, we used the AWS Schema Conversion Tool in the previous section.\n"
},
{
	"uri": "/7-cleanup/7.2-deletedmstask/",
	"title": "Delete the Database Migration Task",
	"tags": [],
	"description": "",
	"content": " Head to the DMS console.\nOn the left-hand menu click on Database migration tasks, and select the migration tasks that you created one at a time if you created multiple.\nClick on the Actions button on the right-hand side, and then select Stop.\nConfirm that you want to stop the migration task.\nAfter the status of the migration tasks changes to Stopped, click on the Actions button again, and then select Delete.\nConfirm that you want to delete the migration task.\n"
},
{
	"uri": "/5-monitoring/5.2-eventnotifications/",
	"title": "Event Notifications",
	"tags": [],
	"description": "",
	"content": "AWS DMS generates various events at DMS instance \u0026amp; task level which you can subscribe. AWS DMS uses Amazon Simple Notification Service (Amazon SNS) to provide notifications when an AWS DMS event occurs.\nTo create a DMS event notification, first you have to create event subscriptions. To create DMS event subscriptions, Go to the AWS DMS console and click on Event Subscriptions in the left side column. Click on Create event subscription button.\nOn create event subscription page, provide name of subscription. Then choose target for notification. For this lab we will use email topic. Enter topic name and email address. Please make sure that you/your team have access to email address provided here. AWS DMS/SNS will push notifications to given email. Optionally, if you already have a topic created, you can choose option Existing topics and select the topic from the list.\nFor event source, you can either select replication-instance or replication-task. For this lab, we will select replication-task. From Event categories we will choose All event categories and from replication instances we will choose All taks. Then submit event creation by clicking on button Create event subscription.\nOnce event subscription is created, check the status and verify that subscription is Active.\nOnce event subscription is created, navigate to AWS SNS subscriptions console. Now select the subscription created by AWS DMS by verifying topic column. AWS Simple Notification Service (SNS) will send a email with confirmation link to the email address you provided in step 2. Status of subscription will stay in pending confirmation.\nAt this point, you will require to access email sent by AWS SNS and Confirm subscription.\nAs soon as the confirmation is received, status of AWS SNS subscription will change to confirmed. Now you are all set to receive email notifications for AWS DMS events.\nTo test event notifications, lets go ahead and perform some action on DMS replication instance. For example, create new replication instance/ reboot instance/ change replication instance class etc. To verify DMS events created for these activities, on DMS console, select Events from left side column. As in this lab, we have created event notifications at replication-instance level, you should receive email for all events for type replication-instance.\nHere is an example of a DMS event notification email. If you are using personal email address but don’t see email coming to inbox, check if emails are sent to spam folder. When you are using company email address, make sure that firewall is not blocking these notifications.\nYou may repeat above steps to create another event subscription for getting notifications on DMS task events.\n"
},
{
	"uri": "/4-serverless/4.2-scaletesting/4.2.2-monitorupscale/",
	"title": "Monitor CloudWatch Dashboard for upscale event",
	"tags": [],
	"description": "",
	"content": "As the DML load is generated on the source database, monitor the AWS DMS console and CloudWatch dashboard for DMS capacity usage. Once the load exceeds 80%, You will see Scale-up event being triggered.\nCapacity Utilization Graph To access CloudWatch metrics for DMS Serverless Replication, navigate to the AWS DMS console, select \u0026ldquo;Serverless replications\u0026rdquo; in the left sidebar, choose your serverless replication, and look for the Capacity Utilization graph under the Monitoring tab.\nYou can look into more detail by clicking on Maximizing Capacity Utilization.\nOnce the load exceeds 80%, you can check the logs after 10 minutes to confirm that the event has triggered the scaling up of the DCU.\nThis concludes our DMS Serverless Scale-up lab. Please proceed to the next lab to learn how to monitor the scaling-down event.\n"
},
{
	"uri": "/2-selectsource/",
	"title": "Select your DMS source",
	"tags": [],
	"description": "",
	"content": "First we will select our source database and perform the activities related to prepare our source. The source databse options for our workshop include Oracle and SQL Server.\nBefore proceeding further, make sure you have completed the instructions in the Getting Started section that preceeded this chapter.\nTo start the workshop, select one of the following sources / modules for today\u0026rsquo;s workshop:\nOracle Source SQL Server Source "
},
{
	"uri": "/2-selectsource/2.2-sqlsrv/",
	"title": "SQL Server source",
	"tags": [],
	"description": "",
	"content": "Now that you have completed setting up the Getting Started, you are ready to migrate a sample data base.\nThis step-by-step guide demonstrates how you can use AWS Database Migration Service (DMS) to migrate data to SQL Server database running on an Amazon EC2. Additionally, you will configure AWS DMS to capture data changes (CDC) on the source database and replicate them on the target database.\nThe environment for this lab consists of:\nAn EC2 instance that hosts the tools used in this lab such as SQL Server Management Studio (SSMS) or Oracle SQL Developer.\nA SQL Server database running on EC2 instance you\u0026rsquo;ll RDP into\nA DMS target\nIn this exercise you perform the following tasks:\nConnect to the EC2 Instance\nOpen SQL Server Management Studio\nConfigure the Source Database\nSummary\n"
},
{
	"uri": "/6-troubleshooting/6.2-tableerrors/",
	"title": "Table Errors In DMS Task",
	"tags": [],
	"description": "",
	"content": "Table Errors In DMS Task In this lab we will discuss scenario where DMS Full-Load-only task ran into errors. Task status switched to Error.\n"
},
{
	"uri": "/4-serverless/4.2-scaletesting/",
	"title": "Testing DMS Serverless Scaling Operations",
	"tags": [],
	"description": "",
	"content": "In this lab, you will explore how AWS DMS Serverless can automatically scale its capacity based on the load from the source system. The primary objective of this load test is to demonstrate the adaptive scalability of DMS Serverless in real-world scenarios. In production environments, data migration workloads can vary significantly, with periods of high activity requiring increased resources and periods of low activity where resources can be minimized to optimize cost. This dynamic scaling feature helps ensure that migrations continue smoothly, even as the load on the source system fluctuates, without requiring manual intervention for capacity adjustments.\nSocial Media Platform Load Testing with AWS DMS Serverless Imagine you\u0026rsquo;re managing a social media platform where users can post content, upvote or downvote posts, and interact with others. This platform experiences typical periods of activity, but during peak times — like when a viral post hits or a major event unfolds — the traffic surges dramatically. Posts are created, users engage with content, and votes start pouring in.\nTo handle these spikes, the system needs to scale its resources quickly and efficiently. That’s where AWS DMS Serverless comes into play. In this lab, we’ll simulate such a scenario by generating a significant amount of data activity within your on-premise data center and replicating it to AWS for further processing. We will observe how DMS Serverless responds to the increasing load and automatically scales its capacity.\n1. Creating Users, Posts, and Votes We’ll simulate a bustling social media site by creating tables to represent Users, Posts, and Votes:\nThe Users table will hold data such as reputation, profile details, and voting activity.\nThe Posts table will contain content ranging from questions to answers, with attributes like scores and views.\nThe Votes table will capture user interactions with these posts, including upvotes, downvotes, or special voting actions like bounties.\nThis data will be generated in the on-premise data center, mimicking the real-world environment of an enterprise application before cloud migration.\n2. Simulating Traffic Surges A stored procedure will generate thousands of posts, users, and votes at random intervals, mimicking the unpredictable traffic of a live social media platform:\nWe\u0026rsquo;ll simulate the creation of posts and interaction with them by users, including how they upvote and downvote content.\nAs the platform becomes busier, the load on the system increases. We\u0026rsquo;ll trigger multiple sessions of high activity, mimicking peak traffic hours.\nAs this traffic builds up in the on-premise system, AWS DMS Serverless will replicate this data in real-time to the AWS Cloud, ensuring that the most up-to-date information is available for further processing in AWS services like analytics or machine learning.\n3. Replicating Data to AWS The data generated on-premise will be replicated seamlessly to the AWS Cloud using DMS Serverless. DMS Serverless ensures high availability, real-time replication, and auto-scaling, which makes it ideal for handling fluctuating loads.\n4. Monitoring System Response DMS Serverless will automatically scale up its capacity (DCU) as the load on the source system increases. This ensures that the migration process can handle the surge in activity without bottlenecks or slowdowns, regardless of the load on the on-premise system.\n5. Scaling Down When Traffic Subsides Once the load decreases—just like when the viral post cools off and the platform returns to normal—DMS Serverless will automatically reduce its capacity, demonstrating the elasticity of serverless computing and cost optimization.\nThe lab will take approximately 40-50 minutes to complete and will give you hands-on experience with DMS Serverless\u0026rsquo;s auto-scaling capabilities and real-time replication from on-premise systems to AWS.\n"
},
{
	"uri": "/6-troubleshooting/6.1-memorypressure/6.1.2-troubleshootingsteps/",
	"title": "Troubleshooting Steps",
	"tags": [],
	"description": "",
	"content": "Regardless of cause of failures, troubleshooting should follow below steps:\nRefresh the DMS task page \u0026amp; verify DMS task status. If task is in Failed status, that means, DMS has stopped processing of task and it is stopped with failures.\nWhen task is in Failed state, verify if there is any message sent by the DMS task. You can view this under Last failure message on the DMS task page. In this scenario, it is very clear that task is failed due to out of memory issue.\nNow, to troubleshoot out of memory issue, navigate to DMS replication instance page and switch to CloudWatch metrics tab. Review metrics specific to Memory \u0026amp; Swap Usage.\nIn above screenshots, we can clearly notice drop in freeable memory \u0026amp; increase in swap usage around the time of task failures. CloudWatch metrics are averaged to 60 seconds. Hence you may not see memory going down completely to zero.\nWhen a DMS task stop cleanly, you may notice below message in DMS task logs:\n2021-11-10T23:58:14 [AT_GLOBAL ]I: Closing log file at Wed Nov 10 23:58:14 2021 (at_logger.c:2548) However, when task fails due to memory pressure, DMS doesn’t get time to cleanly stop the task and write logs. Hence if you refer to DMS task logs in these scenarios, you might not find any useful information for troubleshooting.\nIn case of memory pressure on DMS replication instance, you may also want to verify how many DMS tasks are running on specific instance. To verify this, navigate to DMS replication instances page where on top right you will notice section called Associated migration tasks. To get a list of all DMS task running on that instance, switch to Migration tasks tab. At this section, you can review list of task with their status.\n"
},
{
	"uri": "/6-troubleshooting/6.2-tableerrors/6.2.2-troubleshootingsteps/",
	"title": "Troubleshooting Steps",
	"tags": [],
	"description": "",
	"content": " When you investigate any DMS migration issues, start your investigation by reviewing current status of task. To review task status, navigate to the AWS DMS console and select the DMS task from the list. You will notice status of each task in the list.\nDMS task status Running with errors or Error means one or more tables in the task is moved to error state. When DMS mark any table in error state, by default, that table get suspended from the migration and DMS stop migration for that table but continue migration for other tables in scope. Whenever table error occur, DMS log warnings and related messages in DMS task logs. Hence to troubleshoot the issue further, open DMS task logs.\nBy default, you will see latest messages in DMS task logs and if you have enabled debug logging on the task, it may become difficult to find exact messages related to table error. In that case you may want to use filtering on DMS task log to filter out related messages. Once you filter for the specific messages for tables in error, you will notice log messages something similar to below log snipped:\nThere can be several reasons where DMS suspend a table from processing. In many cases, you might not find enough information from DMS task logs to troubleshoot the issue further. In those cases, best next step would be to enable debug logging on the task and reload the suspended table. Once DMS reprocess the table and put that again in suspended mode, review new DMS task logs which will have more detailed logs.\nIn our scenario, from DMS task logs you may notice (a) DMS throwing error when trying to load data into **\u0026quot;DMS_SAMPLE\u0026quot;.\u0026quot;SPORTING_EVENT_TICKET\u0026quot;**. (b) DMS suspended table due to failure in loading the data to target table. However, DMS logs were not conclusive to identify exact cause of failure and why data loading on target table failed. However, please do note the timestamp when the table was marked as suspended.\rNow, to troubleshoot the issue further and identify cause of failure in data loading on target table, we will investigate target database logs. Navigate to the RDS console. From left side column, select Databases and then click on the target database. In this scenario we are using Aurora PostgreSQL database as target, hence we will select writer instance of Aurora cluster. Navigate to Logs \u0026amp; events tab.\nYou may notice many log files generated under the Logs section of target database. Please review timestamp we noted from previous step and select the right database log file around that time. In same cases you might require to review more than one log file to identify events in database logs. To open Aurora PostgreSQL database log, select the log file from the list and click on the View button.\nOnce you have database logs open, filter events in log related to table in error state. You may notice events related to load failure due to duplicate key violation as we can see in below log snipped:\nFrom the database logs, now it is very clear why the DMS task error out loading the table. When we first created the DMS task, it already loaded data from source to target database. To create this scenario, when we restarted the task, DMS tried to load same data from source to target again. However, target table have primary key which prevented duplicate records to get loaded and hence rejected the records which eventually failed the copy commanded used by DMS for loading data into Aurora PostgreSQL database. Due to failure of this copy command, DMS suspended loading that table and turned task status as Error.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.1-configtarget/c-aurorapostgres/",
	"title": "Aurora (PostgresSQL compatible) target",
	"tags": [],
	"description": "",
	"content": " Open pgAdmin from within the EC2 server task bar at the bottom of the screen (look for elephant), and click on that/open it. Close the warning about a new version of pgAdmin if it pops up. Note the order of icons on your instance maybe different than image below.\nClick on Add Server \u0026gt; New server button or on word Servers in the left pane then right Click and highlight Register then highlight Server and create a new database connection for the target Aurora PostgreSQL database using the following values. You will need to complete General tab and Connection tab separately. Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the password. It is also on the lab\u0026rsquo;s Cloudformation Stack\u0026rsquo;s Output tab:\nParameter Value General -\u0026gt; Name Target Aurora RDS (PostgreSQL) Connection -\u0026gt; Host Name/Address \u0026lt; TargetAuroraPostgreSQLEndpoint \u0026gt; you can find this on Cloudformation \u0026gt; stack \u0026gt; output tab or you can goto RDS in the AWS console and find your Postgresql Target instance Connection -\u0026gt; Port 5432 Connection -\u0026gt; Maintenance Database AuroraDB Connection -\u0026gt; Username dbadmin Connection -\u0026gt; Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for Postgresql Connection -\u0026gt; Save Password Check or turn on/slide on After that, you can click Save and it will connect you.\nRight-click on the AuroraDB database from left-hand menu, and then select Query Tool. Note: you can click through the list of objects on the left to see your converted tables if you\u0026rsquo;d like but not required.\nIn this step you are going to drop the foreign key constraint from the target database for full load to occur since DMS isn\u0026rsquo;t aware of constraints:\nIn this step you are going to drop the foreign key constraint from the target database. This step varies by source database so find your source database below and follow the steps appropriately. Note you do not need to run scripts for sources that you didn\u0026rsquo;t select earlier:\nFor Oracle Source expand this\rCopy the content of the sql code below and paste it into the Query Editor in pgAdmin and Execute the script. (note that there\u0026rsquo;s a copy button to right hand corner to copy the code block)\nALTER TABLE dms_sample.player DROP CONSTRAINT IF EXISTS sport_team_fk; ALTER TABLE dms_sample.seat DROP CONSTRAINT IF EXISTS s_sport_location_fk; ALTER TABLE dms_sample.seat DROP CONSTRAINT IF EXISTS seat_type_fk; ALTER TABLE dms_sample.sport_division DROP CONSTRAINT IF EXISTS sd_sport_type_fk; ALTER TABLE dms_sample.sport_division DROP CONSTRAINT IF EXISTS sd_sport_league_fk; ALTER TABLE dms_sample.sport_league DROP CONSTRAINT IF EXISTS sl_sport_type_fk; ALTER TABLE dms_sample.sport_team DROP CONSTRAINT IF EXISTS st_sport_type_fk; ALTER TABLE dms_sample.sport_team DROP CONSTRAINT IF EXISTS home_field_fk; ALTER TABLE dms_sample.sporting_event DROP CONSTRAINT IF EXISTS se_sport_type_fk; ALTER TABLE dms_sample.sporting_event DROP CONSTRAINT IF EXISTS se_away_team_id_fk; ALTER TABLE dms_sample.sporting_event DROP CONSTRAINT IF EXISTS se_home_team_id_fk; ALTER TABLE dms_sample.sporting_event DROP CONSTRAINT IF EXISTS se_location_id_fk; ALTER TABLE dms_sample.sporting_event_ticket DROP CONSTRAINT IF EXISTS set_person_id; ALTER TABLE dms_sample.sporting_event_ticket DROP CONSTRAINT IF EXISTS set_sporting_event_fk; ALTER TABLE dms_sample.sporting_event_ticket DROP CONSTRAINT IF EXISTS set_seat_fk; ALTER TABLE dms_sample.ticket_purchase_hist DROP CONSTRAINT IF EXISTS tph_sport_event_tic_id; ALTER TABLE dms_sample.ticket_purchase_hist DROP CONSTRAINT IF EXISTS tph_ticketholder_id; ALTER TABLE dms_sample.ticket_purchase_hist DROP CONSTRAINT IF EXISTS tph_transfer_from_id; For SQL Server Source expand this\rWithin PGAdmin, click on the Tools menu, and choose Query Tool.\nNext, click the open file button as shown below If you used classic SCT then paste something like \u0026ldquo;C:\\Users\\Administrator\\AWS Schema Conversion Tool\\Projects\u0026rdquo; into the path window hit enter/click then select the folder with the name you used for your project when setting up SCT such as AWS Schema Conversion Tool SQL Server to Aurora PostgreSQL or Project 1, then select Amazon Aurora sub folder then select the drop foreign key constraints file similar to the screen shot below\nIf you used the in console SC to convert schema, you\u0026rsquo;ll need to go in pgadmin and manually drop the foreign keys from the following tables manually - player, seat, sport_division, sport_league, sport_team, sporting_event, sporting_event_ticket \u0026amp; ticket_purchase_hist. The reason for this is SC generates a unique key on the end of FK when SQL Server is source and PG is target and SC doesn\u0026rsquo;t generate the drop FK script as of this time.\nNext, in the popup window, you should see a list of files similar to the last step in the Schema Convert module if not the project folder may need to be adjusted, select the drop foreign key constraints file similar to screen shot below\nNext, if necessary Copy the content to pgAdmin query editor and adjust for your schema name with a search and replace.\nNext, execute the script. You should see either a successful alter table/drop constraint or skipping constraint doesn\u0026rsquo;t exist both are fine. Note if you\u0026rsquo;d like you can go check one of the tables like player and confirm there are not any constraints there but not required.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/c-aurorapostgres/",
	"title": "Aurora (PostgresSQL compatible) target",
	"tags": [],
	"description": "",
	"content": "In this page, you will inspect the target database after migration.\nOpen pgAdmin4 from within the EC2 server, and then connect to the Target Aurora RDS (PostgreSQL) database connection that you created earlier.\nInspect the migrated data, by querying one of the tables in the target database. For example, the following query should return a table with two rows:\nFor Oracle Source expand this\rSELECT * FROM dms_sample.sport_type; For SQL Server Source expand this\rSELECT * FROM dms_sample_dbo.sport_type; Baseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\nNow, use the following script to enable the foreign key constraints that we dropped earlier. Copy the content of the file to the Query Editor in pgAdmin 4 then Execute the script. (note there\u0026rsquo;s a copy button to right hand corner to copy the code block)\nFor Oracle Source expand this\rALTER TABLE dms_sample.player ADD CONSTRAINT sport_team_fk FOREIGN KEY (sport_team_id) REFERENCES dms_sample.sport_team(id) ON DELETE CASCADE; ALTER TABLE dms_sample.seat ADD CONSTRAINT seat_type_fk FOREIGN KEY (seat_type) REFERENCES dms_sample.seat_type(name) ON DELETE CASCADE; /* Skipping because of long wait time for the query to complete ALTER TABLE dms_sample.seat ALTER COLUMN sport_location_id TYPE numeric; ALTER TABLE dms_sample.seat ADD CONSTRAINT s_sport_location_fk FOREIGN KEY (sport_location_id) REFERENCES dms_sample.sport_location(id) ON DELETE CASCADE; */ ALTER TABLE dms_sample.sport_division ADD CONSTRAINT sd_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_division ADD CONSTRAINT sd_sport_league_fk FOREIGN KEY (sport_league_short_name) REFERENCES dms_sample.sport_league (short_name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_league ADD CONSTRAINT sl_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name); ALTER TABLE dms_sample.sport_team ADD CONSTRAINT st_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample.sport_team ADD CONSTRAINT home_field_fk FOREIGN KEY (home_field_id) REFERENCES dms_sample.sport_location (id) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample.sport_type (name); ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_away_team_id_fk FOREIGN KEY (away_team_id) REFERENCES dms_sample.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event ADD CONSTRAINT se_home_team_id_fk FOREIGN KEY (home_team_id) REFERENCES dms_sample.sport_team (id); ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_person_id FOREIGN KEY(ticketholder_id) REFERENCES dms_sample.person (ID) ON DELETE CASCADE; ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_sporting_event_fk FOREIGN KEY (sporting_event_id) REFERENCES dms_sample.sporting_event (id) ON DELETE CASCADE; /* Skipping because of long wait time for the query to complete ALTER TABLE dms_sample.sporting_event_ticket ALTER COLUMN sport_location_id TYPE numeric; ALTER TABLE dms_sample.sporting_event_ticket ADD CONSTRAINT set_seat_fk FOREIGN KEY (sport_location_id, seat_level, seat_section, seat_row, seat) REFERENCES dms_sample.seat (sport_location_id, seat_level, seat_section, seat_row, seat); */ ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_sport_event_tic_id FOREIGN KEY (sporting_event_ticket_id) REFERENCES dms_sample.sporting_event_ticket (id) ON DELETE CASCADE; ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_ticketholder_id FOREIGN KEY (purchased_by_id) REFERENCES dms_sample.person (ID); ALTER TABLE dms_sample.ticket_purchase_hist ADD CONSTRAINT tph_transfer_from_id FOREIGN KEY (transferred_from_id) REFERENCES dms_sample.person (ID); For SQL Server Source expand this\rALTER TABLE dms_sample_dbo.player ADD CONSTRAINT sport_team_fk FOREIGN KEY (sport_team_id) REFERENCES dms_sample_dbo.sport_team(id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.seat ADD CONSTRAINT seat_type_fk FOREIGN KEY (seat_type) REFERENCES dms_sample_dbo.seat_type(name) ON DELETE CASCADE; /* Skipping because of long wait time for the query to complete ALTER TABLE dms_sample_dbo.seat ALTER COLUMN sport_location_id TYPE numeric; ALTER TABLE dms_sample_dbo.seat ADD CONSTRAINT s_sport_location_fk FOREIGN KEY (sport_location_id) REFERENCES dms_sample_dbo.sport_location(id) ON DELETE CASCADE; */ ALTER TABLE dms_sample_dbo.sport_division ADD CONSTRAINT sd_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_division ADD CONSTRAINT sd_sport_league_fk FOREIGN KEY (sport_league_short_name) REFERENCES dms_sample_dbo.sport_league (short_name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_league ADD CONSTRAINT sl_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name); ALTER TABLE dms_sample_dbo.sport_team ADD CONSTRAINT st_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sport_team ADD CONSTRAINT home_field_fk FOREIGN KEY (home_field_id) REFERENCES dms_sample_dbo.sport_location (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_sport_type_fk FOREIGN KEY (sport_type_name) REFERENCES dms_sample_dbo.sport_type (name); ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_away_team_id_fk FOREIGN KEY (away_team_id) REFERENCES dms_sample_dbo.sport_team (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event ADD CONSTRAINT se_home_team_id_fk FOREIGN KEY (home_team_id) REFERENCES dms_sample_dbo.sport_team (id); ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_person_id FOREIGN KEY(ticketholder_id) REFERENCES dms_sample_dbo.person (ID) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_sporting_event_fk FOREIGN KEY (sporting_event_id) REFERENCES dms_sample_dbo.sporting_event (id) ON DELETE CASCADE; /* Skipping because of long wait time for the query to complete ALTER TABLE dms_sample_dbo.sporting_event_ticket ALTER COLUMN sport_location_id TYPE numeric; ALTER TABLE dms_sample_dbo.sporting_event_ticket ADD CONSTRAINT set_seat_fk FOREIGN KEY (sport_location_id, seat_level, seat_section, seat_row, seat) REFERENCES dms_sample_dbo.seat (sport_location_id, seat_level, seat_section, seat_row, seat); */ ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_sport_event_tic_id FOREIGN KEY (sporting_event_ticket_id) REFERENCES dms_sample_dbo.sporting_event_ticket (id) ON DELETE CASCADE; ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_ticketholder_id FOREIGN KEY (purchased_by_id) REFERENCES dms_sample_dbo.person (ID); ALTER TABLE dms_sample_dbo.ticket_purchase_hist ADD CONSTRAINT tph_transfer_from_id FOREIGN KEY (transferred_from_id) REFERENCES dms_sample_dbo.person (ID); You should see a query returned successfully window\nBaseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\n"
},
{
	"uri": "/3-migratetotarget/3.1-schemaconv/3.1.3-schemaconv/",
	"title": "Convert the Schema",
	"tags": [],
	"description": "",
	"content": "Now that you have created a new Database Migration Project, the next step is to convert the schema of the source database to the Amazon Aurora MYSQL.\nClick on DMS_SAMPLE schema from source on the left hand side of screen\n\u0026gt;\u0026gt; For Oracle Source expand here\r\u0026gt;\u0026gt; For SQL Server Source expand here\rAWS SCT analyses the schema and creates a database migration assessment report for the conversion to PostgreSQL. Items with a red exclamation mark next to them cannot be directly translated from the source to the target. This includes Stored Procedures, and Packages.\nClick on the View button, and choose Assessment Report view.\nFor Oracle source it will look like\nSQL Server will look similar just with SQL Server source on left side of the screen and an SQL Server database hierachy instead of Oracle. You\u0026rsquo;ll want to click on dms_sample in MS SQL Server\nNext, navigate to the Action Items tab in the report to see the items that the tool could not convert, and find out how much manual changes you need to make.\nFor MS SQL Server source, it should look like:\nFor Oracle source, it should look like:\nAWS SCT analyzes the schema and creates a database migration assessment report for the conversion to Aurora MySQL. Items with a blue mark next to them cannot be directly translated from the source to the target. Items in green will be translated over from source to target. In this case, this includes the stored procedures. For each conversion issue, you can complete one of the following actions:\nModify the objects on the source database so that AWS SCT can convert the objects to the target Aurora MySQL database.\nInstead of modifying the source schema, modify scripts that AWS SCT generates before applying the scripts on the target Aurora MySQL database.\nHowever, for the sake of time, we skip modifying all the objects that could not be automatically converted. Instead, as an example, you will manually modify one of the stored procedures from within SCT to make it compatible with the target database.\nThis is demonstrated in this subsection.\nClick on the **dms_sample** for Oracle or dbo for sql server schema in the left-hand panel, and click Convert Schema. For Oracle it will look like below and for other databases similar except that database name at the top and its hierarchy on the left\nYou may be prompted with a dialog box “Object may already exist in the target database, replace?” Select Yes.\nRight click on the dms_sample_dbo (SQL Sever) or dms_sample (other DBs) schema in the right-hand panel, and click Apply to database.\n\u0026gt;\u0026gt; For Oracle Source expand here\r\u0026gt;\u0026gt; For SQL Server Source expand here\rWhen prompted if you want to apply the schema to the database, click Yes.\nYou may see an exclamation mark on certain database objects such as indexes, and foreign key constraints. In the next section we will drop foreign key target database.\nAt this point, the schema has been applied to the target database. Expand the dms_sample_dbo or dms_sample schema on the right hand pane to see the tables, views, procedures, etc. Note please expand per source below as some have extra steps specifically SQL Server requires additional steps.\n\u0026gt;\u0026gt; For SQL Server Source expand here\rIn preparation for future steps we will need to generate sql script for the constraints we just added to the target. To do this, we first want to change the project settings from the Settings menu at top and selecting Project settings as shown below. Select Save scripts from the menu on left then Select Aurora Postgresql from top drop down for Vendor and select Single file for each Stage in the second drop down as shown below.\nNow Click on Aurora Postgresql target right side of your screen so dms_sample_dbo is highlighted and right click so the pop up menu shows and select Save as SQL as shown below.\nSelect the project folder and save the files. Leave this window open as you\u0026rsquo;ll need to come back to it later. It should look similar to below but note things change fairly fast here so if it isn\u0026rsquo;t an exact match that\u0026rsquo;s okay.\nYou have sucessfully converted the database schema and object from source to Amazon Aurora.\nThis part demonstrated how easy it is to migrate the schema of a source database into Amazon Aurora PostgreSQL using the AWS Schema Conversion Tool. Similarly, you learned how the Schema Conversion Tool highlights the differences between different database engine dialects, and provides you with tips on how you can successfully modify the code when needed to migrate procedure and other database objects.\nThe same steps can be followed to migrate SQL Server and Oracle workloads to other RDS/Aurora engines including PostgreSQL and MySQL.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.3-createendpoint/",
	"title": "Create DMS Source and Target Endpoints",
	"tags": [],
	"description": "",
	"content": "Now that you have a replication instance, you need to create source and target endpoints for the sample database.\nClick on the Endpoints link on the left, and then click on Create endpoint on the top right corner.\nEnter the following information to create an endpoint for the source dms_sample database Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the SQL Server password (same as earlier if you\u0026rsquo;ve already done this). It is also on first Cloudformation Stack\u0026rsquo;s output tab under password either SQL Server or Oracle depending on your source:\nOracle source: Expand this if your source DB type is Oracle\rParameter Value Endpoint Type Source endpoint Select RDS DB instance Checked for Oracle others leave blank RDS Instance -SourceOracleDB Endpoint Identifier oracle-source or take populated value Descriptive Amazon Resource Name leave blank Source Engine oracle Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name \u0026lt; SourceOracleEndpoint Should auto populate \u0026gt; Port 1521 SSL Mode none User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for Oracle Database Name dms_sample_target Test endpoint connection -\u0026gt; VPC Replication Instance cfn-DMSReplication or DMSReplication or the one you created if it is ready Microsoft SQL Server source: Expand this if your source DB type is Microsoft SQL Server\rParameter Value Endpoint Type Source endpoint Endpoint Identifier sqlserver-source Descriptive Amazon Resource Name leave blank Source Engine Microsoft SQL Server Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name \u0026lt; EC2 ip address/domain name \u0026gt; - Look it up in the Output tab of the CFn stack Port 1433 SSL Mode none User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretPSQLServer or look it up in AWS Secrets Manager for SQL Server Database Name dms_sample Test endpoint connection -\u0026gt; VPC Replication Instance cfn-DMSReplication or DMSReplication or the one you created if it is ready Endpoint Settings unfold arrow, check \u0026ldquo;Use Endpoint connection attributes\u0026rdquo; then add setUpMsCdcForTables=true to the text box Once the information has been entered, select the existing DMS Replcation instance (the one without your initials) and click Run Test. When the status turns to successful, click Create endpoint.\nFollow the same steps to create another endpoint for the Target Aurora RDS Database (dms_sample_dbo or dms_sample depending on your source db) using the following values Please note the password is not provided below you need to goto Secrets Manager and open DMSDBSecret and reveal the SQL Server password. It is also on first Cloudformation Stack\u0026rsquo;s output tab under SQL Server password (yes this is mysql but the passwords are the same for sql server \u0026amp; mysql in this case) :\nAurora (MySQL) target: Expand this if your target type is Aurora (MySQL)\rParameter Value Endpoint Type Target endpoint Select RDS DB instance Checked for all DB targets Select RDS DB instance -AuroraMySQLInstance Endpoint Identifier aurora-mysql-target Descriptive Amazon Resource Name leave blank Target Engine Amazon Aurora MySQL Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name **TargetAuroraMySQLEndpoint** or it should auto populate Port 3306 SSL Mode none User Name **dbadmin** Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager Test endpoint connection -\u0026gt; VPC Replication Instance **cfn-DMSReplication** or **DMSReplication** or the one you created if it is ready Aurora (PostgreSQL) target: Expand this if your target type is Aurora (PostgreSQL)\rParameter Value Endpoint Type Target endpoint Select RDS DB instance Checked for all DB targets Select RDS DB instance -AuroraMySQLInstance Endpoint Identifier **aurora-postgresql-target** or it should auto populate Descriptive Amazon Resource Name leave blank Target Engine Amazon Aurora PostgreSQL Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name **TargetAuroraPostgreSQLEndpoint** or it should auto populate Port 5432 SSL Mode none User Name **dbadmin** Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager Test endpoint connection -\u0026gt; VPC Replication Instance **cfn-DMSReplication** or **DMSReplication** or the one you created if it is ready RDS Microsoft SQL Server target: Expand this if your target type is RDS Microsoft SQL Server:\rParameter Value Endpoint Type Target endpoint Select RDS DB instance Checked for all DB targets Select RDS DB instance -TargetSQLServer Endpoint Identifier **sqlserver-target** or use name that auto populates Descriptive Amazon Resource Name leave blank Target Engine Microsoft SQL Server Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name TargetSqlServerEndpoint (this should auto populate) Port 1433 SSL Mode none User Name **dbadmin** Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager that ends in RDSDBSecret Database Name **dms_sample_target** Test endpoint connection -\u0026gt; VPC \u0026lt;VPC ID with DMSVpc in the name from Environment Setup Step\u0026gt; Replication Instance **cfn-dmsreplication** or just **dmsreplication** RDS Oracle target: Expand this if your target type is Oracle:\rParameter Value Endpoint Type Target endpoint Select RDS DB instance Checked all DB targets Select RDS DB instance \u0026lt; StackName \u0026gt;-TargetOracleDB Endpoint Identifier oracle-target or take the default that auto populates Descriptive Amazon Resource Name (if there) leave blank Target Engine oracle Access to Endpoint database Radio Button Select/click Provide access information Manually Server Name \u0026lt; TargetOracleEndpoint \u0026gt; should auto populate Port 1521 SSL Mode none User Name **dbadmin** Password See Cloudformation Output tab \u0026amp; **DMSDBSecretP** or look it up in AWS Secrets Manager for Oracle Database Name **TARGETDB** Test endpoint connection -\u0026gt; VPC \u0026lt;VPC ID with DMSVpc in the name from Environment Setup Step\u0026gt; Replication Instance **cfn-dmsreplication** or just **dmsreplication** Amazon S3 Target Expand this if your target type is Amazon S3:\rParameter Value Endpoint Type Target endpoint Endpoint Identifier **S3-target** Descriptive Amazon Resource Name leave blank Target Engine Amazon S3 Server Name \u0026lt; ARN of the **dms-migration-role** \u0026gt; Bucket Name \u0026lt; Name of Your S3 Bucket \u0026gt; Bucket folder **dmstargetfolder** Once the information has been entered, select the existing DMS Replcation instance (the one without your initials) and click Run Test. When the status turns to successful, click Create endpoint.\n"
},
{
	"uri": "/7-cleanup/7.3-deleteendpoints/",
	"title": "Delete the DMS Endpoints",
	"tags": [],
	"description": "",
	"content": " Still in the Database Migration Service console, click on Endpoints in the left-hand menu\nSelect the endpoints that you created as a part of the workshop. Then, click on the Actions button on the right-hand side, and choose Delete.\nConfirm that you want to delete the endpoints.\nContinue to the next page to delete the DMS Replication instance\u0026hellip;\n"
},
{
	"uri": "/4-serverless/4.2-scaletesting/4.2.3-monitordownscale/",
	"title": "Monitor CloudWatch Dashboard for downscale event",
	"tags": [],
	"description": "",
	"content": "Unlike scaling up, which aggressively responds to increased load on the source server, scaling down occurs more gradually. Scaling down process takes some time to adjust the capacity, allowing for a smoother transition as the demand on the source system decreases.\nOnce the CDC load has been catchup, the cooling period is for an hour where DMS serverless task triggers the scale down event of a DCU.\nDMS serverless task is now scaled down to 4 DCU\nOutput of CloudWatch logs showing Serverless replication scaling down from 8 DCU to 4 DCU\nYou have now completed the serverless module. If you are running in your own account be sure to complete the event clean up section Resource and Environment Cleanup.\n"
},
{
	"uri": "/1-start/1.3-cfnenv/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "",
	"content": "In this step, you will use a CloudFormation (CFN) template to deploy the infrastructure for this database migration. AWS CloudFormation simplifies provisioning the infrastructure, so we can concentrate on tasks related to data migration.\nOpen the AWS CloudFormation console , and click on the Create Stack button.\nSelect Template is ready, and choose Upload a template file as the source template. Then, click on Choose file and upload the DMSWorkshop2025.yaml. Click Next.\nPopulate the form as with the values specified below, and then click Next.\nInput Parameter Values Stack Name A unique identifier without spaces. MigrationType / Lab Type Database that you want to migrate (Oracle or SQL Server). EC2ServerInstanceType An Amazon EC2 Instance type from the drop-down menu. Recommend using the default value. KeyName The KeyPair (DMSKeypair) that you created in the previous step. RDSInstanceType An Amazon RDS Instance type from the drop-down menu. Recommend using the default value. VpcCIDR The VPC CIDR range in the form x.x.x.x/16. Defaults to 10.20.0.0/16 IAMRoleDmsVpcExist Does your AWS account already have dms-vpc-role(goto IAM\u0026gt;roles \u0026amp; search for \u0026ldquo;dms-vpc\u0026rdquo; to check) if this role is not there template will fail-rollback unless you change default Y to N? The resources that are created here will be prefixed with whatever value you specify in the Stack Name. Please specify a value that is unique to your account.\nOn the Stack Options page, accept all of the defaults and click Next.\nOn the Review page, click Create stack.\nAt this point, you will be directed back to the CloudFormation console and will see a status of CREATE_IN_PROGRESS. Please wait here until the status changes to CREATE_COMPLETE.\nOnce CloudFormation status changes to CREATE_COMPLETE, go to the Outputs section.\nMake a note of the Output values from the CloudFormation environment that you launched as you will need them for the remainder of the tutorial:\n"
},
{
	"uri": "/6-troubleshooting/6.1-memorypressure/6.1.3-resolution/",
	"title": "Resolution",
	"tags": [],
	"description": "",
	"content": "To resolve DMS task failure due to memory pressure, we have two options\nOption 1. Increase memory on DMS replication instance. You can increase memory by modifying DMS instance and selecting instance class with higher memory. Please refer to this AWS DMS document to select right instance type for your workload.\nOption 2. Reduce the workload. In this lab, we were using DMS instance type t2.micro which gives 1 GB of memory. At the start of this lab, to create this scenario, we increased workload by adding more tables \u0026amp; schemas. We also increased number of tables to load in parallel.\nHence, in our case, to reduce the workload, you may (a) reduce number of tables loading in parallel. (b) reduce number of schemas/tables. (c) If there are multiple task running in parallel on same DMS instance, you may want to move few task to another instances.\n"
},
{
	"uri": "/6-troubleshooting/6.2-tableerrors/6.2.3-resolution/",
	"title": "Resolution",
	"tags": [],
	"description": "",
	"content": "Now when we know the root cause of failure, resolution is easy part in this scenario. To resolve the duplicate key violation so you can reload the data from source database, you will require to connect to target database and truncate all tables which are in the Error state.\ntruncate table dms_sample.sport_location; truncate table dms_sample.sport_division; truncate table dms_sample.sport_league; truncate table dms_sample.sport_type; truncate table dms_sample.sporting_event; truncate table dms_sample.sporting_event_ticket; truncate table dms_sample.sport_team; truncate table dms_sample.seat_type; truncate table dms_sample.seat; truncate table dms_sample.player; truncate table dms_sample.name_data; Alternatively, you can also modify DMS task setting TargetTablePrepMode from DO_NOTHING to TRUNCATE_BEFORE_LOAD. This way, DMS can truncate the target table before loading the data.\n"
},
{
	"uri": "/3-migratetotarget/",
	"title": "Select your DMS Target",
	"tags": [],
	"description": "",
	"content": "Now that you have completed setting up the Getting Started and picking your source database, you are ready to migrate a sample data base.\nThis step-by-step guide demonstrates how you can use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (DMS) to convert schema and migrate data to Amazon Aurora (MySQL/PostgreSQL). Additionally, you will use AWS DMS to continually replicate database changes from the source database to the target database.\nThe environment for this lab consists of:\nA data source like SQL Server or Oracle.\nAn EC2 instance with Schema Conversion Tool and other database gui tools, as the source database.\nAn Amazon Aurora/RDS instance used as the target database.\nAn S3 Bucket if you choose S3 as your target\n"
},
{
	"uri": "/5-monitoring/5.3-tablestatistics/",
	"title": "Table Statistics",
	"tags": [],
	"description": "",
	"content": "The AWS DMS console updates information regarding the state of your tables during migration.\nTo view table statistics of your DMS task, Go to the AWS DMS console and click on Replication Tasks in the left side column. Click on the task from the list in right side.\nOn DMS task page, navigate to Table statistics tab. Here you will find statistics for all tables in scope of DMS migration task.\nDescription of each column in table statistics is discussed here.\n"
},
{
	"uri": "/1-start/1.4-ec2connect/",
	"title": "Connect to the EC2 Instance and install the Schema Conversion tool",
	"tags": [],
	"description": "",
	"content": "Connect to the EC2 instance There are two methods to connect to the EC2-Instance:\nThrough the traditional RDP client, which is more complex and requires more steps and\nThrough AWS Fleet Manager, which is easier and requires fewer steps and is our recommended approach.\nThough the instructions below cover both, we suggest you start with Fleet Manager approach which is listed first.\nIf you are using a browser where copy and paste won\u0026rsquo;t work, we suggest opening the lab instructions in a browser inside the RDP window with fleet manager to enable copy and paste.\nFleet Manager Remote Desktop approach Expand to see the instructions\r(RDP Approach is after this section if you have chosen that way. You can skip to that section by clicking here.)\nGo to the AWS Fleet Manger console and click on the instance with a name that ends with -EC2Instance (if you start on the Fleet Manager dashboard click the Getting Started Button) then select Node actions button and select Connect with Remote Desktop as show in the image below (if you get a something went wrong message or don\u0026rsquo;t see an instance make sure you are in the correct region for your event)\nSelect the Key pair radio button then select Browse to find the key pair downloaded to your local machine. You should have downloaded the key pair earlier (covered in the Getting started section under download key pair). If you have not, go to Key pair in the Amazon EC2 Console and download it now.\nClick Connect.\nClick No on the right sidebar Networks \u0026gt; Network 10 when asked Do you want to allow your PC to be discoverable\u0026hellip;.? on the right if it shows up.\nClassic Desktop RDP connection approach Expand to see the instructions\r(We will use Remote Desktop Protocol (RDP) to connect to the Amazon EC2 Instance.)\nIf you are using OS X, you can get Microsoft Remote Desktop from app store here.\nGo to the AWS EC2 console and click on Security Groups in the left column (you may have to scroll down some).\nSelect the security group that ends in -InstanceSecurityGroup by checking the box then click Actions \u0026gt; Edit \u0026gt; Inbound rules as shown below.\nEdit the inbound rules for RDP by clicking Source and selecting My IP as shown below. Click Save rules (Note this is following best practices if you have a VPN or complex network you may need to come back and change this to Anywhere IPv4 0.0.0.0/0 but we\u0026rsquo;ll start with this)\nGo to the AWS EC2 console and click on Instances in the left column.\nSelect the instance with the name -EC2Instance and then click the Actions button. Click on Connect.\nGo to the RDP client section, and click on Get Password.\nClick on Browse and upload the private key file that you downloaded earlier.\nClick on Decrypt Password.\nCopy the generated password and save it somewhere. You will use this password to connect to login to the EC2 instance.\nClick on Download Remote Desktop File to download the RDP file to access this EC2 instance.\nConnect to the EC2 instance using a RDP client.\nInstall the schema conversion tool Reconnected the EC2 instance we used to prepare the source database via rdp or Fleet Manager, we will install the AWS Schema Conversion tool on this server. Downloading the file and installing it will give you the latest version of the AWS Schema Conversion Tool.\nOn the EC2 server, open the \u0026ldquo;DMS Workshop\u0026rdquo; folder that is on the Desktop. Then, double-click on \u0026ldquo;AWS Schema Conversion Tool Download\u0026rdquo; to get the latest version of the software like picture below. (Note: if it asks do you want to open this file say \u0026ldquo;YES\u0026rdquo;.)\nWhen the download is complete, unzip the content (double click lower left corner of browser where download occurred or click once and select Open) double click on the install AWS Schema windows install package file and follow the install wizard steps taking the defaults and complete the install the AWS Schema Conversion Tool by clicking finish once done. (like the images below).\nNote: Fleet Mgr double click can be tricky. Single click and hit enter key with the file highlighted works better some times. If you see Compressed Folder error pop up close this window and try the single click enter key approach vs. double click with mouse. Also sometimes the install wizard gets covered up so see if it is on the task bar at bottom and click on it there it looks like windows installation icon at bottom of RDP screen. If you still have issues ask your AWS contact/instructor.\nThe example installation process is in the following pictures:\nWhen the installer is complete, the installation dialog will disappear. There is no other notification.\nOnce the installation is complete, open the Start Menu and launch the AWS Schema Conversion Tool or you can double click on the SCT orange box icon on the desktop.\nAccept the Terms and Conditions.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.4-createtask/",
	"title": "Create a DMS Migration Task",
	"tags": [],
	"description": "",
	"content": "AWS DMS uses Database Migration Tasks to migrate the data from source to the target database.\nClick on Database migration tasks on the left-hand menu, then click on the Create task button on the top right corner.\nCreate a data migration task with the following values for migrating the **dms_sample** database.\nExpand this if your target type is Aurora (MySQL)\rParameter Value Task identifier source-to-AuroraMySQL-target Replication instance cfn-dmsreplication or just dmsreplication Source database endpoint your source database (SQL Server, Oracle, etc.) from prior step Target database endpoint aurora-target or cfn-auroramysqlinstance or whatever you named it in prior step Migration type Migrate existing data and replicate ongoing changes CDC stop mode Don’t use custom CDC stop mode Create recovery table on target DB leave blank/unchecked Target table preparation mode Do nothing (not the default) Stop task after full load completes Don’t stop LOB Column settings / Include LOB columns in replication Limited LOB mode Max LOB size (KB) 32 Data Validation Unchecked Task Logs / Enable CloudWatch logs Check Enable CloudWatch logs (not the default) Log Context leave checked take default levels on logging Batch-optimized apply if visible Turn on batched optimized apply -\u0026gt; Unchecked Expand this if your target type is Aurora (PostgreSQL)\rParameter Value Task identifier source-to-aupg-migration-task Replication instance cfn-dmsreplication or just dmsreplication Source database endpoint your source database (SQL Server, Oracle, etc.) from prior step Target database endpoint aurora-target or cfn-aurorapostgresql* or whatever you named it in prior step Migration type Migrate existing data and replicate ongoing changes CDC stop mode Don’t use custom CDC stop mode Create recovery table on target DB leave blank/unchecked Target table preparation mode Do nothing (not the default) Stop task after full load completes Don’t stop LOB Column settings / Include LOB columns in replication Limited LOB mode Max LOB size (KB) 32 Data Validation Unchecked Task Logs / Enable CloudWatch logs Check Enable CloudWatch logs (not the default) Log Context leave checked take default levels on logging Batch-optimized apply if visible Turn on batched optimized apply -\u0026gt; Unchecked Expand this if your target type is RDS Microsoft SQL Server:\rParameter Value Task identifier source-to-sqlserver-target Replication instance cfn-dmsreplication or just dmsreplication Source database endpoint your source database SQL Server or Oracle etc from prior step Target database endpoint sqlserver-target or whatever you named it in prior step Migration type Migrate existing data and replicate ongoing changes CDC stop mode Don’t use custom CDC stop mode Create recovery table on target DB leave blank/unchecked Target table preparation mode Do nothing (not the default) Stop task after full load completes Don’t stop LOB Column settings / Include LOB columns in replication Limited LOB mode Max LOB size (KB) 32 Data Validation Unchecked Task Logs / Enable CloudWatch logs Check Enable CloudWatch logs (not the default) Log Context leave checked take default levels on logging Batch-optimized apply if visible Turn on batched optimized apply -\u0026gt; Unchecked Expand this if your target type is Oracle:\rParameter Value Task identifier source-to-oracle-target Replication instance cfn-dmsreplication or just dmsreplication Source database endpoint your source database SQL Server or Oracle etc from prior step Target database endpoint oracle-target or whatever you named it in prior step Migration type Migrate existing data and replicate ongoing changes CDC stop mode Don’t use custom CDC stop mode Create recovery table on target DB leave blank/unchecked Target table preparation mode Do nothing (not the default) Stop task after full load completes Don’t stop LOB Column settings / Include LOB columns in replication Limited LOB mode Max LOB size (KB) 32 Data Validation Unchecked Task Logs / Enable CloudWatch logs Check Enable CloudWatch logs (not the default) Log Context leave checked take default levels on logging Batch-optimized apply if visible Turn on batched optimized apply -\u0026gt; Unchecked Expand this if your target type is Amazon S3:\rParameter Value Task identifier source-to-s3-target Replication instance cfn-dmsreplication or just dmsreplication Source database endpoint your source database SQL Server or Oracle etc from prior step Target database endpoint s3-target or whatever you named it in prior step Migration type Migrate existing data and replicate ongoing changes CDC stop mode Don’t use custom CDC stop mode Create recovery table on target DB leave blank/unchecked Target table preparation mode Do nothing (not the default) Stop task after full load completes Don’t stop LOB Column settings / Include LOB columns in replication Limited LOB mode Max LOB size (KB) 32 Data Validation Unchecked Task Logs / Enable CloudWatch logs Check Enable CloudWatch logs (not the default) Log Context leave checked take default levels on logging Batch-optimized apply if visible Turn on batched optimized apply Unchecked Expand the Table mappings section, and select Wizard for the editing mode.\nClick on Add new selection rule button and enter the following values in the form:\nFor Oracle source, use: Parameter Value Schema DMS_SAMPLE% Table name % Action Include For SQL Server source, use: Parameter Value Schema dbo% Table name % Action Include If the Create Task screen does not recognize any schemas, make sure to go back to endpoints screen and click on your endpoint. Scroll to the bottom of the page and click on Refresh Button (⟳) in the Schemas section. If your schemas still do not show up on the Create Task screen, click on the Guided tab and manually select **dbo** schema and all tables.\nNext, expand the Transformation rules section, and click on Add new transformation rule using the following values:\nFor Oracle source, use:\nRule 1: Parameter Value Target Schema Schema Name DMS_SAMPLE Action Make lowercase Rule 2: Parameter Value Target Table Schema Name DMS_SAMPLE Table Name % Action Make lowercase Rule 3: Parameter Value Target Column Schema Name DMS_SAMPLE Table Name % Column Name % Action Make lowercase For SQL Server Source, use:\nParameter Value Target Schema Schema Name % (if Microsoft SQL is used) or %dbo Action Rename to: dms_sample_dbo or dms_sample_target Uncheck Turn on premigration assessment this should collapse this section for our workshop this part there isn\u0026rsquo;t time for and it isn\u0026rsquo;t required.\nAfter entering the values, make sure Migration task startup configuration is set to start Automatically on create, then click on Create task.\nAt this point, the task should start running and replicating data from the **dms_sample** database running on your source to the Amazon Aurora RDS (PostgreSQL) instance. Note the DMS task will take a moment to create (Status=creating), then it will transition into Status=Ready, followed by Status=Starting, and then Status=Running. You can use the circular arrow refresh button to update the status for the task.\n"
},
{
	"uri": "/7-cleanup/7.4-deleterepinstance/",
	"title": "Delete the DMS Replication Instance",
	"tags": [],
	"description": "",
	"content": " Still in the Database Migration Service console, click on Replication instances in the left-hand menu\nSelect the replication instance that you created as a part of the workshop. Then, click on the Actions button on the right-hand side, and choose Delete.\nConfirm that you want to delete the replication instance.\nContinue to the next page to delete the rest of the resources\u0026hellip;\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/d-oracle/",
	"title": "Oracle target",
	"tags": [],
	"description": "",
	"content": "In this page, you will inspect the target database after migration.\nGo back to the EC2 instance we used earlier, once connected, open Oracle SQL Developer, and click the green button to add a new connection using the following values:\nParameter Value Connection Name Target Oracle User Name dbadmin Password See Cloudformation Output tab \u0026amp; DMSDBSecretP or look it up in AWS Secrets Manager for Oracle target Save Password Check Hostname \u0026lt; TargetOracle Endpoint (you can find this in Cloudformation Output tab or goto RDS service in console and pick your RDS Oracle Target instance\u0026gt; Port 1521 SID/Service Name TargetDB Inspect the migrated data, by querying one of the tables in the target database. For example, the following query should return a table with two rows:\nBaseball, and football are the only two sports that are currently listed in this table. In the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target database.\n"
},
{
	"uri": "/4-serverless/",
	"title": "Serverless replication",
	"tags": [],
	"description": "",
	"content": "AWS DMS Serverless eliminates the guesswork and operational burden of managing migration resources by automatically provisioning, scaling, and optimizing capacity. It offers built-in high availability and a pay-for-use billing model, allowing you to begin migrations quickly with minimal oversight while optimizing costs and increasing operational agility. This feature also removes the need for manual tasks such as capacity estimation, provisioning, cost management, and replication engine maintenance.\nIn this module, we will demonstrate:\nHow to use the AWS DMS Serverless feature for data migration. You\u0026rsquo;ll start by setting up a DMS Serverless to perform a full load and CDC operation.\nAfter that, you\u0026rsquo;ll generate some activity on the source database to observe how DMS Serverless automatically scales in response to the workload.\nBy the end of this module, you will have hands-on experience in configuring and managing DMS Serverless for efficient and automated data migration.\nBefore proceeding with the instructions in this module, please ensure that you have completed all the prerequisite steps, including Getting Started section and configuring your source and target databases. If you have skipped any of these foundational modules, it\u0026rsquo;s essential to go back and complete them first. Skipping these prerequisites may result in issues or incomplete setups during the data migration process. For a smooth and successful migration, make sure all necessary configurations are in place by clicking below links.\n"
},
{
	"uri": "/5-monitoring/5.4-tasklogs/",
	"title": "Task Logs",
	"tags": [],
	"description": "",
	"content": "AWS DMS integrated with AWS CloudWatch Service to log task information during the migration process. You have option to enable logging on each DMS task. Once it is enabled, logs are continuously published through cloudWatch logs.\nTo view DMS task logs, Go to the AWS DMS console and click on Replication Tasks in the left side column. Click on the task from the list in right side. Once you are on DMS task page, look for button View logs on top right corner. You may also open task logs from Overview details tab on the same page.\nOnce you click on View logs, it will open a new window under CloudWatch service. By default, you will see latest logs on the screen. On the bottom of the page, you will have a resume button. Once you click that, logs will auto scroll when new logs are generated.\nSometimes you may require to filter the logs to search for specific message in the log. You can do it by using filtering pattern under the Log events.\nIt is recommended to always enable DMS task logging at the time of creation. You would require additional permissions on AWS CloudWatch service to view the DMS tasks.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/e-s3/",
	"title": "Amazon S3 Target",
	"tags": [],
	"description": "",
	"content": " Go to your target S3 bucket and click on dmstargetfolder. Click on dms_sample_dbo or dms_sample subfolder. Here you will find all the tables that were migrated from the source database.\nGo to the sport_type folder. Click the checkbox on the left of the .CSV object and download the file.\nOpen the CSV file on your computer and note that baseball and football are the only two sports that are currently listed in this table.\nIn the next section you will insert several new records to the source database with information about other sport types. DMS will automatically replicate these new records from the source database to the target S3 bucket.\n"
},
{
	"uri": "/7-cleanup/7.5-deletecfnstack/",
	"title": "Delete the CloudFormation Stack",
	"tags": [],
	"description": "",
	"content": " Next, go to the CloudFormation console, and click select the CloudFormation Stack that you created during the workshop.\nClick on the Delete button from the top right corner. CloudFormation will automatically remove all resources that it launched earlier. This process can take up to 15 minutes.\nConfirm that you want to delete the stack.\nCheck the CloudFormation console to ensure the stack that you selected is removed.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.5-inspect/",
	"title": "Inspect the Content of Target Database",
	"tags": [],
	"description": "",
	"content": "We use different methods to inspect your target database, depending on its schema:\nMicrosoft SQL Server target\rAurora (MySQL compatible) target\rAurora (PostgresSQL compatible) target\rOracle target\rAmazon S3 Target\r"
},
{
	"uri": "/5-monitoring/",
	"title": "Monitoring DMS Migrations",
	"tags": [],
	"description": "",
	"content": "Monitoring is an important part of maintaining the reliability, availability, and performance of AWS DMS and your AWS solutions. AWS provides several tools for monitoring your AWS DMS tasks and resources, and responding to potential incidents. This section will demonstrate how you can use those tools with AWS Database Migration Service (DMS).\n"
},
{
	"uri": "/5-monitoring/5.5-taskstatus/",
	"title": "Task Status",
	"tags": [],
	"description": "",
	"content": "AWS DMS refresh task status as \u0026amp; when the task transition from one state to another. To view status of your DMS tasks, Go to the AWS DMS console and click on DMS Migration Tasks on left side column. On the right side, you will see list of all DMS tasks with their current status.\nThe following table shows the possible statuses a task can have:\nCreating : AWS DMS is creating the task.\nRunning : The task is performing the migration duties specified.\nStopped : The task is stopped.\nStopping : The task is being stopped. This is usually an indication of user intervention in the task.\nDeleting : The task is being deleted, usually from a request for user intervention.\nFailed : The task has failed. For more information, see the task log files.\nStarting : The task is connecting to the replication instance and to the source and target endpoints. Any filters and transformations are being applied.\nReady : The task is ready to run. This status usually follows the \u0026ldquo;creating\u0026rdquo; status.\nModifying : The task is being modified, usually due to a user action that modified the task settings.\nMoving : The task is in the process of being moved to another replication instance. The replication remains in this state until the move is complete. Deleting the task is the only operation allowed on the replication task while it’s being moved.\nFailed-move : The task move has failed for any reason, such as not having enough storage space on the target replication instance. When a replication task is in this state, it can be started, modified, moved, or deleted.\n"
},
{
	"uri": "/7-cleanup/7.6-deletebucketandrole/",
	"title": "Delete S3 Bucket and IAM Roles",
	"tags": [],
	"description": "",
	"content": "\rThis section should be completed only if you did the Amazon S3 target lab. You can STOP reading now if you completed any of the other labs in the Database Migration Workshop because you have completed removing the resources for those labs at this stage.\nNavigate to Amazon S3 console and go to the bucket that you created as part of this lab.\nClick on the checkbox left to the **dmstargetfolder**. Click on the Actions button, and select Empty.\nGet back to the bucket list and click on the checkbox next to the S3 bucket you created earlier. Click on Delete.\nType in the name of your bucket, then click on Delete.\nThe lab uses the **DMS-VPC-ROLE** which is needed for DMS to work in general. We would recommend leaving this role in place as there is no cost for IAM Roles and DMS will need this role if it doesn\u0026rsquo;t exist. If you have any questions or concerns please reach out to your AWS contacts or support.\nNow, you need to delete the IAM policy that you created earlier. Note: you could leave it as there\u0026rsquo;s no cost for an IAM policy but the bucket used has been deleted. It is up to you. If you want to keep it you are done with this section. If you want to delete it, go to the IAM console , and then go to Policies from the navigation pane. In the search bar, type in **DMS-LAB-S3-Access-Policy**.\nClick on the checkbox next to the **DMS-LAB-S3-Access-Policy**. Then, click on Policy actions and select Delete.\nSelect Delete on the new prompt.\nYou have completed removing the AWS resources that you created earlier in your account.\n"
},
{
	"uri": "/3-migratetotarget/3.2-migration/3.2.6-replicatechange/",
	"title": "Replicate Data Changes",
	"tags": [],
	"description": "",
	"content": "Now you are going to simulate a transaction to the source database by updating the sport_type table. The Database Migration Service will automatically detect and replicate these changes to the target database.\nFor Oracle Source:\nExpand this if your source DB type is Oracle\rOn the EC2 instance you RDP-ed into, use Oracle SQL Developer connect to the source Oracle RDS.\nOpen a New Query window and execute the following statement to insert 5 new sports into the sport_type table:\nINSERT ALL INTO dms_sample.sport_type (name,description) VALUES (\u0026#39;hockey\u0026#39;, \u0026#39;A sport in which two teams play against each other by trying to more a puck into the opponents goal using a hockey stick\u0026#39;) INTO dms_sample.sport_type (name,description) VALUES (\u0026#39;basketball\u0026#39;, \u0026#39;A sport in which two teams of five players each that oppose one another shoot a basketball through the defenders hoop\u0026#39;) INTO dms_sample.sport_type (name,description) VALUES (\u0026#39;soccer\u0026#39;,\u0026#39;A sport played with a spherical ball between two teams of eleven players\u0026#39;) INTO dms_sample.sport_type (name,description) VALUES (\u0026#39;volleyball\u0026#39;,\u0026#39;two teams of six players are separated by a net and each team tries to score by grounding a ball on the others court\u0026#39;) INTO dms_sample.sport_type (name,description) VALUES (\u0026#39;cricket\u0026#39;,\u0026#39;A bat-and-ball game between two teams of eleven players on a field with a wicket at each end\u0026#39;) SELECT * FROM dual; COMMIT; SELECT * FROM dms_sample.sport_type; Expand this if your source DB type is Microsoft SQL Server\rFor SQL Server Source:\nUse Microsoft SQL Server Management Studio to connect to the Source SQL Server on the EC2 instance.\nOpen a New Query window and execute the following statement to insert 5 new sports into the sport_type table:\nUSE [dms_sample] GO INSERT INTO [dbo].[sport_type] ([name] ,[description]) VALUES (\u0026#39;hockey\u0026#39;, \u0026#39;A sport in which two teams play against each other by trying to more a puck into the opponents goal using a hockey stick\u0026#39;); INSERT INTO [dbo].[sport_type] ([name] ,[description]) VALUES (\u0026#39;basketball\u0026#39;, \u0026#39;A sport in which two teams of five players each that oppose one another shoot a basketball through the defenders hoop\u0026#39;); INSERT INTO [dbo].[sport_type] ([name] ,[description]) VALUES (\u0026#39;soccer\u0026#39;,\u0026#39;A sport played with a spherical ball between two teams of eleven players\u0026#39;); INSERT INTO [dbo].[sport_type] ([name] ,[description]) VALUES (\u0026#39;volleyball\u0026#39;,\u0026#39;two teams of six players are separated by a net and each team tries to score by grounding a ball on the others court\u0026#39;); INSERT INTO [dbo].[sport_type] ([name] ,[description]) VALUES (\u0026#39;cricket\u0026#39;,\u0026#39;A bat-and-ball game between two teams of eleven players on a field with a wicket at each end\u0026#39;); GO Repeat steps from prior screen that selected the contents of sport_type table in the target database.\nThe new records for that you added for basketball, cricket, hockey, soccer, volleyball to the sports_type table in the source database have been replicated to your dms_sample database. You can further investigate the number of inserts, deletes, updates, and DDLs by viewing the Table statistics of your Database migration tasks in AWS console.\n"
},
{
	"uri": "/5-monitoring/5.6-runbook/",
	"title": "RunBook",
	"tags": [],
	"description": "",
	"content": "AWS DMS Monitoring runbook provides detailed instructions on the automated monitoring solution developed for AWS Database Migration Service (DMS). The solution is implemented using Amazon Lambda using Python and AWS SDKs, automating several key monitoring and alerting tasks for entire fleet of DMS resources. The primary focus is on creating a consolidated monitoring and alerting setup to ensure the health and performance of DMS replication instances and tasks.\nThis run book deploys following monitoring:\nCentralized Amazon CloudWatch dashboard to review resource consumption (e. g, CPU, Memory, Storage utilizations or Capacity utilization for DMS Serverless etc.) by all AWS DMS Classic Instances.\nCentralized Amazon CloudWatch dashboard to review CDC (Change Data Capture) Metrics like Source Latency, Target Latency etc. from all DMS tasks.\nSetup AWS DMS event notifications (Including change of state like stop, start, fail etc. for all DMS Instances \u0026amp; tasks) for all AWS DMS classic Instances and tasks.\nSetup hourly Amazon CloudWatch alerts for Errors \u0026amp; Warnings in all AWS DMS migrations including AWS DMS Classic, homogenous migration or DMS Serverless. Users can customize the notification frequency.\nSetup alerts for AWS DMS Instances to notify when breach thresholds.\nSetup AWS DMS best practices alerts for DMS Instances and DMS task. For example, script will alert if there are DMS instances with public access enabled or unused DMS Classic Instances or DMS task with debug logging enabled.\nAll monitoring placed by the solution are fully customizable where users can choose the CloudWatch metrics or task log filtering for special events.\nDeploy one-click solution using this AWS CloudFormation template: AWS Database Migration Monitoring Runbook deployment\nFollow step by step instructions AWS Database Migration Monitoring Runbook github.\n"
},
{
	"uri": "/6-troubleshooting/",
	"title": "Troubleshooting Migrations with AWS DMS",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Migrations with AWS DMS This lab module is designed to give you hands-on experience on DMS task troubleshooting. Each scenario in this lab is different from others and you can pick them in any sequence.\n"
},
{
	"uri": "/7-cleanup/",
	"title": "Environment Cleanup",
	"tags": [],
	"description": "",
	"content": "Environment Cleanup After you have completed the database migration workshop, you need to remove the AWS resources that you created in your account to stop incurring costs. This section walks you through the steps to delete these resources…\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]